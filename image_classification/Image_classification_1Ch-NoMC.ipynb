{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1Ch-NoMC\n",
        "- Input = single slices\n",
        "- Standard Dropout applied: Only at training time, not at test time\n",
        "- 5 fold cross validation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as imgplot\n",
        "import time\n",
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from os.path import exists\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(3004)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for the output\n",
        "output_folder = \"C:/Users/hezo/Dropbox/PhD/Stroke/Stroke_classification/Analyses_Oct_2018/outputs/5fold_CV_bl_without_mc_dropout/\"\n",
        "\n",
        "if not exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    os.makedirs(output_folder + \"/checkpoints\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data from the hdf5 file\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "def decode_data(string):\n",
        "    decoded_string = [n.decode(\"UTF-8\", \"ignore\") for n in string]\n",
        "    return(decoded_string)\n",
        "\n",
        "with h5py.File('data/data_oct_18.h5', 'r') as h5:\n",
        "    print('H5-file: ', list(h5.keys()))\n",
        "    \n",
        "    # Image matrices\n",
        "    X = h5[\"X\"][:]\n",
        "    # Image labels (1=stroke, 0=no-stroke)\n",
        "    Y_img = h5[\"Y\"][:]\n",
        "    # Patient ID's\n",
        "    pat = h5[\"pat\"][:]\n",
        "    # Path to images\n",
        "    path = decode_data(h5[\"path\"][:])\n",
        "    # Image names/number\n",
        "    img = decode_data(h5[\"img_id\"][:])\n",
        "    # Patient labels (1=stroke, 0=TIA)\n",
        "    Y_pat = h5[\"stroke\"][:]\n",
        "    \n",
        "print(len(X), len(Y_img), len(Y_pat), len(pat), len(path), len(img))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove black images"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the functions that are used to remove the black images\n",
        "from functions.get_quantiles import get_quantiles\n",
        "\n",
        "# Help function to delete images by index\n",
        "def delete_by_index(X, Y_img, Y_pat, pat, path, img, idx):\n",
        "    X = np.delete(X, idx, axis=0) \n",
        "    Y_img = np.delete(Y_img, idx, axis=0)\n",
        "    pat = np.delete(pat, idx, axis=0)\n",
        "    path = np.delete(path, idx, axis=0)\n",
        "    img = np.delete(img, idx, axis=0)\n",
        "    Y_pat = np.delete(Y_pat, idx, axis=0)\n",
        "    return(X, Y_img, Y_pat, pat, path, img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before: ', len(X), len(Y_img), len(Y_pat), len(pat), len(path), len(img))\n",
        "\n",
        "# Get the 1st and 99th quantiles\n",
        "q1, q99 = get_quantiles(X)\n",
        "\n",
        "# If the 1st and 99th quantil are equivalent --> image is black\n",
        "idx_black_img = np.where(q99[:,0]==q1[:,0])\n",
        "X, Y_img, Y_pat, pat, path, img = delete_by_index(X, Y_img, Y_pat, pat, path, img, idx_black_img)\n",
        "q1=np.delete(q1, idx_black_img, axis=0) \n",
        "q99=np.delete(q99, idx_black_img, axis=0) \n",
        "\n",
        "# If the 99th quantile is smaller 10 --> image is black\n",
        "idx_black_img=np.where((q99[:,0]<10))\n",
        "X, Y_img, Y_pat, pat, path, img = delete_by_index(X, Y_img, Y_pat, pat, path, img, idx_black_img)\n",
        "q1=np.delete(q1, idx_black_img, axis=0) \n",
        "q99=np.delete(q99, idx_black_img, axis=0) \n",
        "\n",
        "print('After: ', len(X), len(Y_img), len(Y_pat), len(pat), len(path), len(img))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define datasets for Cross Validation\n",
        "Split the indices of stroke and no-stroke patients, such that the percentage of stroke and no-stroke patients is the same within each dataset. We have 355 stroke patients (out of 511) ~70%. We use the following splits for test1,...,test4\n",
        "- train1: 196 stroke, 83 no-stroke\n",
        "- valid1: 35 stroke, 15 no-stroke\n",
        "- valid2: 54 stroke, 26 no-stroke\n",
        "- test: 71 stroke, 31 no-stroke\n",
        "\n",
        "for test5 we use:\n",
        "- train1: 196 stroke, 82 no-stroke\n",
        "- valid1: 35 stroke, 15 no-stroke\n",
        "- valid2: 54 stroke, 26 no-stroke\n",
        "- test: 71 stroke, 32 no-stroke"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the seed works and we always get the same results\n",
        "# np.random.seed(1)\n",
        "# random_pat = np.random.choice(np.unique(pat), size=len(np.unique(pat)), replace=False)\n",
        "# print(random_pat[:10])\n",
        "# np.random.seed(1) \n",
        "# random_pat = np.random.choice(np.unique(pat), size=len(np.unique(pat)), replace=False)\n",
        "# print(random_pat[:10])\n",
        "# np.random.seed(1) \n",
        "# random_pat = np.random.choice(np.unique(pat), size=len(np.unique(pat)), replace=False)\n",
        "# print(random_pat[:10])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider stroke and no-stroke patients separately\n",
        "idx = np.where(Y_pat == 1)[0]\n",
        "stroke_patients = np.unique(pat[idx])\n",
        "idx = np.where(Y_pat == 0)[0]\n",
        "non_stroke_patients = np.unique(pat[idx])\n",
        "print(len(stroke_patients), len(non_stroke_patients))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly shuffle indices of stroke and no-stroke patients\n",
        "np.random.seed(1)\n",
        "stroke_patients_test = np.random.choice(stroke_patients, size=len(stroke_patients), replace=False)\n",
        "non_stroke_patients_test = np.random.choice(non_stroke_patients, size=len(non_stroke_patients), replace=False)\n",
        "print(stroke_patients_test[:3])\n",
        "\n",
        "# define the different test sets:\n",
        "# 71 stroke and 31 no-stroke patients within each test set\n",
        "test_tmp_1 = np.concatenate([stroke_patients_test[:71], non_stroke_patients_test[:31]], axis=0)\n",
        "test_tmp_2 = np.concatenate([stroke_patients_test[71:142], non_stroke_patients_test[31:62]], axis=0)\n",
        "test_tmp_3 = np.concatenate([stroke_patients_test[142:213], non_stroke_patients_test[62:93]], axis=0)\n",
        "test_tmp_4 = np.concatenate([stroke_patients_test[213:284], non_stroke_patients_test[93:124]], axis=0)\n",
        "# the last dataset contains 32 no-stroke\n",
        "test_tmp_5 = np.concatenate([stroke_patients_test[284:355], non_stroke_patients_test[124:156]], axis=0)\n",
        "\n",
        "# randomly shuffle the data sets such that stroke and no-stroke patients are mixed\n",
        "test_1 = np.random.choice(test_tmp_1, size=len(test_tmp_1), replace=False)\n",
        "test_2 = np.random.choice(test_tmp_2, size=len(test_tmp_2), replace=False)\n",
        "test_3 = np.random.choice(test_tmp_3, size=len(test_tmp_3), replace=False)\n",
        "test_4 = np.random.choice(test_tmp_4, size=len(test_tmp_4), replace=False)\n",
        "test_5 = np.random.choice(test_tmp_5, size=len(test_tmp_5), replace=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_1 # looks good"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### RUN 1\n",
        "\n",
        "# get the patients that are not conatined in test1\n",
        "stroke_patients_run = [i for i in stroke_patients if i not in test_1]\n",
        "non_stroke_patients_run = [i for i in non_stroke_patients if i not in test_1]\n",
        "\n",
        "# randomply shuffle the data\n",
        "np.random.seed(100)\n",
        "stroke_patients_tmp = np.random.choice(stroke_patients_run, size=len(stroke_patients_run), replace=False)\n",
        "non_stroke_patients_tmp = np.random.choice(non_stroke_patients_run, size=len(non_stroke_patients_run), replace=False)\n",
        "print(len(stroke_patients_tmp), len(non_stroke_patients_tmp))\n",
        "\n",
        "# take the patients for the different datasets\n",
        "# - train1: 196 stroke, 83 no-stroke\n",
        "# - valid1: 35 stroke, 15 no-stroke\n",
        "# - valid2: 54 stroke, 26 no-stroke\n",
        "# - test: 71 stroke, 31 no-stroke\n",
        "train1_tmp = np.concatenate([stroke_patients_tmp[0:196],non_stroke_patients_tmp[:83]], axis=0)\n",
        "valid1_tmp = np.concatenate([stroke_patients_tmp[196:231], non_stroke_patients_tmp[83:98]], axis=0)\n",
        "train2_tmp = np.concatenate([train1_tmp,valid1_tmp], axis=0)\n",
        "valid2_tmp = np.concatenate([stroke_patients_tmp[231:285], non_stroke_patients_tmp[98:124]], axis=0)\n",
        "print(len(train1_tmp), train1_tmp[:10])\n",
        "print(len(valid1_tmp), valid1_tmp[:10])\n",
        "print(len(train2_tmp), train2_tmp[:10])\n",
        "print(len(valid2_tmp), valid2_tmp[:10])\n",
        "\n",
        "# randomly shuffle the datasets such that stroke and no-stroke patients are mixed\n",
        "train1_1 = np.random.choice(train1_tmp, size=len(train1_tmp), replace=False)\n",
        "valid1_1 = np.random.choice(valid1_tmp, size=len(valid1_tmp), replace=False)\n",
        "train2_1 = np.random.choice(train2_tmp, size=len(train2_tmp), replace=False)\n",
        "valid2_1 = np.random.choice(valid2_tmp, size=len(valid2_tmp), replace=False)\n",
        "test_1 = np.random.choice(test_1, size=len(test_1), replace=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### RUN 2\n",
        "\n",
        "# get the patients that are not conatined in test2\n",
        "stroke_patients_run = [i for i in stroke_patients if i not in test_2]\n",
        "non_stroke_patients_run = [i for i in non_stroke_patients if i not in test_2]\n",
        "\n",
        "# randomply shuffle the data\n",
        "np.random.seed(200)\n",
        "stroke_patients_tmp = np.random.choice(stroke_patients_run, size=len(stroke_patients_run), replace=False)\n",
        "non_stroke_patients_tmp = np.random.choice(non_stroke_patients_run, size=len(non_stroke_patients_run), replace=False)\n",
        "print(len(stroke_patients_tmp), len(non_stroke_patients_tmp))\n",
        "\n",
        "# take the patients for the different datasets\n",
        "# - train1: 196 stroke, 83 no-stroke\n",
        "# - valid1: 35 stroke, 15 no-stroke\n",
        "# - valid2: 54 stroke, 26 no-stroke\n",
        "# - test: 71 stroke, 31 no-stroke\n",
        "train1_tmp = np.concatenate([stroke_patients_tmp[0:196],non_stroke_patients_tmp[:83]], axis=0)\n",
        "valid1_tmp = np.concatenate([stroke_patients_tmp[196:231], non_stroke_patients_tmp[83:98]], axis=0)\n",
        "train2_tmp = np.concatenate([train1_tmp,valid1_tmp], axis=0)\n",
        "valid2_tmp = np.concatenate([stroke_patients_tmp[231:285], non_stroke_patients_tmp[98:124]], axis=0)\n",
        "print(len(train1_tmp), train1_tmp[:10])\n",
        "print(len(valid1_tmp), valid1_tmp[:10])\n",
        "print(len(train2_tmp), train2_tmp[:10])\n",
        "print(len(valid2_tmp), valid2_tmp[:10])\n",
        "\n",
        "# randomly shuffle the datasets such that stroke and no-stroke patients are mixed\n",
        "train1_2 = np.random.choice(train1_tmp, size=len(train1_tmp), replace=False)\n",
        "valid1_2 = np.random.choice(valid1_tmp, size=len(valid1_tmp), replace=False)\n",
        "train2_2 = np.random.choice(train2_tmp, size=len(train2_tmp), replace=False)\n",
        "valid2_2 = np.random.choice(valid2_tmp, size=len(valid2_tmp), replace=False)\n",
        "test_2 = np.random.choice(test_2, size=len(test_2), replace=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### RUN 3\n",
        "\n",
        "# get the patients that are not conatined in test3\n",
        "stroke_patients_run = [i for i in stroke_patients if i not in test_3]\n",
        "non_stroke_patients_run = [i for i in non_stroke_patients if i not in test_3]\n",
        "\n",
        "# randomply shuffle the data\n",
        "np.random.seed(300)\n",
        "stroke_patients_tmp = np.random.choice(stroke_patients_run, size=len(stroke_patients_run), replace=False)\n",
        "non_stroke_patients_tmp = np.random.choice(non_stroke_patients_run, size=len(non_stroke_patients_run), replace=False)\n",
        "print(len(stroke_patients_tmp), len(non_stroke_patients_tmp))\n",
        "\n",
        "# take the patients for the different datasets\n",
        "# - train1: 196 stroke, 83 no-stroke\n",
        "# - valid1: 35 stroke, 15 no-stroke\n",
        "# - valid2: 54 stroke, 26 no-stroke\n",
        "# - test: 71 stroke, 31 no-stroke\n",
        "train1_tmp = np.concatenate([stroke_patients_tmp[0:196],non_stroke_patients_tmp[:83]], axis=0)\n",
        "valid1_tmp = np.concatenate([stroke_patients_tmp[196:231], non_stroke_patients_tmp[83:98]], axis=0)\n",
        "train2_tmp = np.concatenate([train1_tmp,valid1_tmp], axis=0)\n",
        "valid2_tmp = np.concatenate([stroke_patients_tmp[231:285], non_stroke_patients_tmp[98:124]], axis=0)\n",
        "print(len(train1_tmp), train1_tmp[:10])\n",
        "print(len(valid1_tmp), valid1_tmp[:10])\n",
        "print(len(train2_tmp), train2_tmp[:10])\n",
        "print(len(valid2_tmp), valid2_tmp[:10])\n",
        "\n",
        "# randomly shuffle the datasets such that stroke and no-stroke patients are mixed\n",
        "train1_3 = np.random.choice(train1_tmp, size=len(train1_tmp), replace=False)\n",
        "valid1_3 = np.random.choice(valid1_tmp, size=len(valid1_tmp), replace=False)\n",
        "train2_3 = np.random.choice(train2_tmp, size=len(train2_tmp), replace=False)\n",
        "valid2_3 = np.random.choice(valid2_tmp, size=len(valid2_tmp), replace=False)\n",
        "test_3 = np.random.choice(test_3, size=len(test_3), replace=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### RUN 4\n",
        "\n",
        "# get the patients that are not conatined in test4\n",
        "stroke_patients_run = [i for i in stroke_patients if i not in test_4]\n",
        "non_stroke_patients_run = [i for i in non_stroke_patients if i not in test_4]\n",
        "\n",
        "# randomply shuffle the data\n",
        "np.random.seed(400)\n",
        "stroke_patients_tmp = np.random.choice(stroke_patients_run, size=len(stroke_patients_run), replace=False)\n",
        "non_stroke_patients_tmp = np.random.choice(non_stroke_patients_run, size=len(non_stroke_patients_run), replace=False)\n",
        "print(len(stroke_patients_tmp), len(non_stroke_patients_tmp))\n",
        "\n",
        "# take the patients for the different datasets\n",
        "# - train1: 196 stroke, 83 no-stroke\n",
        "# - valid1: 35 stroke, 15 no-stroke\n",
        "# - valid2: 54 stroke, 26 no-stroke\n",
        "# - test: 71 stroke, 31 no-stroke\n",
        "train1_tmp = np.concatenate([stroke_patients_tmp[0:196],non_stroke_patients_tmp[:83]], axis=0)\n",
        "valid1_tmp = np.concatenate([stroke_patients_tmp[196:231], non_stroke_patients_tmp[83:98]], axis=0)\n",
        "train2_tmp = np.concatenate([train1_tmp,valid1_tmp], axis=0)\n",
        "valid2_tmp = np.concatenate([stroke_patients_tmp[231:285], non_stroke_patients_tmp[98:124]], axis=0)\n",
        "print(len(train1_tmp), train1_tmp[:10])\n",
        "print(len(valid1_tmp), valid1_tmp[:10])\n",
        "print(len(train2_tmp), train2_tmp[:10])\n",
        "print(len(valid2_tmp), valid2_tmp[:10])\n",
        "\n",
        "# randomly shuffle the datasets such that stroke and no-stroke patients are mixed\n",
        "train1_4 = np.random.choice(train1_tmp, size=len(train1_tmp), replace=False)\n",
        "valid1_4 = np.random.choice(valid1_tmp, size=len(valid1_tmp), replace=False)\n",
        "train2_4 = np.random.choice(train2_tmp, size=len(train2_tmp), replace=False)\n",
        "valid2_4 = np.random.choice(valid2_tmp, size=len(valid2_tmp), replace=False)\n",
        "test_4 = np.random.choice(test_4, size=len(test_4), replace=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### RUN 5\n",
        "\n",
        "# get the patients that are not conatined in test5\n",
        "stroke_patients_run = [i for i in stroke_patients if i not in test_5]\n",
        "non_stroke_patients_run = [i for i in non_stroke_patients if i not in test_5]\n",
        "\n",
        "# randomply shuffle the data\n",
        "np.random.seed(500)\n",
        "stroke_patients_tmp = np.random.choice(stroke_patients_run, size=len(stroke_patients_run), replace=False)\n",
        "non_stroke_patients_tmp = np.random.choice(non_stroke_patients_run, size=len(non_stroke_patients_run), replace=False)\n",
        "print(len(stroke_patients_tmp), len(non_stroke_patients_tmp))\n",
        "\n",
        "# take the patients for the different datasets\n",
        "# - train1: 196 stroke, 83 no-stroke\n",
        "# - valid1: 35 stroke, 15 no-stroke\n",
        "# - valid2: 54 stroke, 26 no-stroke\n",
        "# - test: 71 stroke, 31 no-stroke\n",
        "train1_tmp = np.concatenate([stroke_patients_tmp[0:196],non_stroke_patients_tmp[:83]], axis=0)\n",
        "valid1_tmp = np.concatenate([stroke_patients_tmp[196:231], non_stroke_patients_tmp[83:98]], axis=0)\n",
        "train2_tmp = np.concatenate([train1_tmp,valid1_tmp], axis=0)\n",
        "valid2_tmp = np.concatenate([stroke_patients_tmp[231:285], non_stroke_patients_tmp[98:124]], axis=0)\n",
        "print(len(train1_tmp), train1_tmp[:10])\n",
        "print(len(valid1_tmp), valid1_tmp[:10])\n",
        "print(len(train2_tmp), train2_tmp[:10])\n",
        "print(len(valid2_tmp), valid2_tmp[:10])\n",
        "\n",
        "# randomly shuffle the datasets such that stroke and no-stroke patients are mixed\n",
        "train1_5 = np.random.choice(train1_tmp, size=len(train1_tmp), replace=False)\n",
        "valid1_5 = np.random.choice(valid1_tmp, size=len(valid1_tmp), replace=False)\n",
        "train2_5 = np.random.choice(train2_tmp, size=len(train2_tmp), replace=False)\n",
        "valid2_5 = np.random.choice(valid2_tmp, size=len(valid2_tmp), replace=False)\n",
        "test_5 = np.random.choice(test_5, size=len(test_5), replace=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize images\n",
        "- make sure that image values range between -1 and 1"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 3\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(n_images):\n",
        "    fig = plt.subplot(1,n_images,i+1)\n",
        "    fig.imshow(X[1,:,:,i], cmap='gray')\n",
        "print(np.min(X[1]), np.max(X[1]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import preprocess_input\n",
        "X_norm = X\n",
        "X_norm = preprocess_input(X_norm, mode='tf')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(X_norm), np.max(X_norm))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(n_images):\n",
        "    fig = plt.subplot(1,n_images,i+1)\n",
        "    fig.imshow(X_norm[0,:,:,i], cmap='gray')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and predict with the CNN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_datasets(set_i, X, Y_img, Y_pat, pat, path, img):\n",
        "    Y_img_set = []\n",
        "    Y_pat_set = []\n",
        "    pat_set = []\n",
        "    path_set = []\n",
        "    img_set = []\n",
        "    # Find the indices corresponding to the patient_i in set_i\n",
        "    idx = [i for i, pat_i in enumerate(pat) if pat_i in set_i]\n",
        "    X_set = X[idx,:,:,:]\n",
        "    for i in idx:\n",
        "        Y_img_set.append(Y_img[i])\n",
        "        Y_pat_set.append(Y_pat[i])\n",
        "        pat_set.append(pat[i])\n",
        "        path_set.append(path[i])\n",
        "        img_set.append(img[i])     \n",
        "    return(X_set, np.array(Y_img_set), np.array(Y_pat_set), np.array(pat_set), np.array(path_set), np.array(img_set))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the CNN and the function for prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize hyperparameters\n",
        "input_shape = (X_norm.shape[1], X_norm.shape[2], X_norm.shape[3])\n",
        "batch_size = 64\n",
        "n_epochs = 400"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Lambda, Convolution2D, MaxPooling2D, Reshape, concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import initializers\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# define the model\n",
        "print('Define model')\n",
        "\n",
        "def conv_block2_all_dropout(input_x, size, dropout_level):\n",
        "    x = Convolution2D(size, (3,3), kernel_initializer=initializers.he_normal(seed=3004), padding='same')(input_x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_level)(x)\n",
        "    x = Convolution2D(size, (3,3), kernel_initializer=initializers.he_normal(seed=3004), padding='same')(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_level)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def conv_block3_all_dropout(input_x, size, dropout_level):\n",
        "    x = Convolution2D(size, (3,3), kernel_initializer=initializers.he_normal(seed=3004), padding='same')(input_x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_level)(x)\n",
        "    x = Convolution2D(size, (3,3), kernel_initializer=initializers.he_normal(seed=3004), padding='same')(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_level)(x)\n",
        "    x = Convolution2D(size, (3,3), kernel_initializer=initializers.he_normal(seed=3004), padding='same')(x)\n",
        "    x = BatchNormalization(axis=3)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dropout_level)(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x) \n",
        "    return x\n",
        "\n",
        "\n",
        "def cnn_all_dropout(input_shape):\n",
        "    \n",
        "    drop_level = 0.3\n",
        "    img_input = Input(shape=input_shape)\n",
        "    \n",
        "    # Convolutional part\n",
        "    x = conv_block2_all_dropout(img_input,32,dropout_level=drop_level)\n",
        "    x = conv_block2_all_dropout(x,64,dropout_level=drop_level)\n",
        "    x = conv_block3_all_dropout(x,128,dropout_level=drop_level)\n",
        "    x = conv_block3_all_dropout(x,256,dropout_level=drop_level)\n",
        "    x = conv_block3_all_dropout(x,512,dropout_level=drop_level)\n",
        "    x = conv_block3_all_dropout(x,512,dropout_level=drop_level)\n",
        "    \n",
        "    # Dense part\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(400, kernel_initializer=initializers.he_normal(seed=3004))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(drop_level)(x)\n",
        "    x = Dense(100, kernel_initializer=initializers.he_normal(seed=3004))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(drop_level)(x)\n",
        "    x = Dense(2, kernel_initializer=initializers.he_normal(seed=3004), activation='softmax')(x)\n",
        "    \n",
        "    model = Model(img_input, x)\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_all_dropout((X.shape[1],X.shape[2],X.shape[3]))\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# The function we use to get the T predictions within the T forward passes through the network with MC Dropout\n",
        "# As we don't apply MC Dropout in this example, we get the same prediction within each run (by using the regular Dropout function in Keras)\n",
        "def get_predictions_cnn_all_dropout(X, Y, output_folder, name, mod, pat, img, stroke, path):\n",
        "    \n",
        "    # Convert labeles back from one hot encoding:\n",
        "    Y = np.argmax(Y, axis=1)\n",
        "            \n",
        "    # start a new session\n",
        "    print('Start new session for predictions')\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    K.set_session(sess)\n",
        "    \n",
        "    # Load the model with the lambda layers (given as function input)\n",
        "    print('Load model cnn_all_dropout with lambda layers with old weights')\n",
        "    model2 = cnn_all_dropout((X.shape[1],X.shape[2],X.shape[3]))\n",
        "    model2.load_weights(output_folder + '/checkpoints/' + mod + '.hdf5')\n",
        "\n",
        "    \n",
        "    # get the predictions\n",
        "    print('Get the predictions')\n",
        "    n_classes = 2\n",
        "    \n",
        "    predictions = np.zeros((len(X), 500, n_classes))\n",
        "    mean0 = np.zeros((X.shape[0]))\n",
        "    mean1 = np.zeros((X.shape[0]))\n",
        "    mean2 = np.zeros((X.shape[0]))\n",
        "    sd0 = np.zeros((X.shape[0]))\n",
        "    sd1 = np.zeros((X.shape[0]))\n",
        "    votes0 = np.zeros((X.shape[0]))\n",
        "    votes1 = np.zeros((X.shape[0]))\n",
        "    total_var = np.zeros((X.shape[0]))\n",
        "    total_sd = np.zeros((X.shape[0]))\n",
        "    vr = []\n",
        "    pe = []\n",
        "    mi = []  \n",
        "    for j in range(len(X)):\n",
        "        # repeat the current image 500 times\n",
        "        X_rep = np.empty((500,X.shape[1],X.shape[2],X.shape[3]))\n",
        "        X_rep[:] = X[j:j+1]\n",
        "        # get 500 predictions for this image\n",
        "        pred = sess.run(model2.output, feed_dict={model2.input: X_rep})\n",
        "        \n",
        "        # output of mean and sd == #classes\n",
        "        predictions[j] = pred # save the raw predictions\n",
        "        mean0[j], mean1[j] = np.mean(pred, axis=0)\n",
        "        votes0[j] = len(np.where(pred[:,0]>=0.5)[0])/500\n",
        "        votes1[j] = len(np.where(pred[:,1]>=0.5)[0])/500\n",
        "        sd0[j], sd1[j] = np.array(np.std(pred, ddof=1, axis=0))\n",
        "        total_var[j] = sd0[j]**2 + sd1[j]**2\n",
        "        total_sd[j] = np.sqrt(sd0[j]**2 + sd1[j]**2)\n",
        "        pred[pred==0]=1e-40\n",
        "        vr.append(1-(np.max(np.histogram(np.argmax(pred, axis=1), bins=n_classes, range=[0,n_classes])[0])/len(pred)))\n",
        "        pe_tmp = (-1)*np.sum(np.mean(pred, axis=0)*np.log(np.mean(pred, axis=0)))\n",
        "        pe.append(pe_tmp)\n",
        "        mi.append(pe_tmp + np.sum(np.array([np.sum(pred[:,i]*np.log(pred[:,i]))for i in range(0,n_classes)]))/len(pred))\n",
        "    \n",
        "    # Save the predictions with additional information\n",
        "    dat = pd.DataFrame({'p_id':pat, 'img':img, 'pat_true':stroke, 'img_true':Y, 'path':path,  \n",
        "                        'mean0':mean0, 'mean1':mean1, 'vr':vr, 'pe':pe, 'mi':mi, 'sd0':sd0, 'sd1':sd1,\n",
        "                        'votes0':votes0, 'votes1':votes1, 'total_var':total_var, 'total_sd':total_sd})\n",
        "    dat.to_csv(output_folder + '/predictions_dropout_' + name + '.csv', index=False)\n",
        "    # save the predictions separately\n",
        "    pred0 = predictions[:,:,0]\n",
        "    pred1 = predictions[:,:,1]\n",
        "    pred0_df = pd.DataFrame(pred0)\n",
        "    pred1_df = pd.DataFrame(pred1)\n",
        "    pred0_df = pred0_df.assign(p_id=pat, img=img, pat_true=stroke, img_true=Y)\n",
        "    pred1_df = pred1_df.assign(p_id=pat, img=img, pat_true=stroke, img_true=Y)\n",
        "    pred0_df.to_csv(output_folder + '/raw_predictions_' + name + '_pred0.csv', index=False)\n",
        "    pred1_df.to_csv(output_folder + '/raw_predictions_' + name + '_pred1.csv', index=False)\n",
        "    np.save(output_folder + '/raw_predictions_dropout_' + name, predictions)\n",
        "    \n",
        "    K.clear_session()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert labels to one hot\n",
        "def convertToOneHot(vector, num_classes=None):\n",
        "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
        "    result[np.arange(len(vector)), vector] = 1\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CV training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train1_cv = np.array([train1_1, train1_2, train1_3, train1_4, train1_5])\n",
        "valid1_cv = np.array([valid1_1, valid1_2, valid1_3, valid1_4, valid1_5])\n",
        "train2_cv = np.array([train2_1, train2_2, train2_3, train2_4, train2_5])\n",
        "valid2_cv = np.array([valid2_1, valid2_2, valid2_3, valid2_4, valid2_5])\n",
        "test_cv = np.array([test_1, test_2, test_3, test_4, test_5])\n",
        "print(train1_cv.shape, valid1_cv.shape, train2_cv.shape, valid2_cv.shape, test_cv.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train1_cv = np.array([train1_1[:2], train1_2[:2], train1_3[:2], train1_4[:2], train1_5[:2]])\n",
        "# valid1_cv = np.array([valid1_1[:2], valid1_2[:2], valid1_3[:2], valid1_4[:2], valid1_5[:2]])\n",
        "# train2_cv = np.array([train2_1[:2], train2_2[:2], train2_3[:2], train2_4[:2], train2_5[:2]])\n",
        "# valid2_cv = np.array([valid2_1[:2], valid2_2[:2], valid2_3[:2], valid2_4[:2], valid2_5[:2]])\n",
        "# test_cv = np.array([test_1[:2], test_2[:2], test_3[:2], test_4[:2], test_5[:2]])\n",
        "# print(train1_cv.shape, valid1_cv.shape, train2_cv.shape, valid2_cv.shape, test_cv.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cv(train1, valid1, train2, valid2, test, X, Y_img, Y_pat, pat, path, img, batch_size, n_epochs):\n",
        "    \n",
        "    # iterate over all runs\n",
        "    for i in range(5):\n",
        "        \n",
        "        print('####### Run ', i, '#######')\n",
        "        \n",
        "        \n",
        "        #### Get the information for one run\n",
        "    \n",
        "        # extract the data\n",
        "        print('Get data')\n",
        "        train1_run = train1[i]\n",
        "        valid1_run = valid1[i]\n",
        "        train2_run = train2[i]\n",
        "        valid2_run = valid2[i]\n",
        "        test_run = test[i]\n",
        "        \n",
        "        # assign the data for the patients in train1_run, etc.\n",
        "        X_train1, Y_img_train1, Y_pat_train1, pat_train1, path_train1, img_train1 = get_datasets(train1_run, X, Y_img, Y_pat, pat, path, img)\n",
        "        X_valid1, Y_img_valid1, Y_pat_valid1, pat_valid1, path_valid1, img_valid1 = get_datasets(valid1_run, X, Y_img, Y_pat, pat, path, img)\n",
        "        X_train2, Y_img_train2, Y_pat_train2, pat_train2, path_train2, img_train2 = get_datasets(train2_run, X, Y_img, Y_pat, pat, path, img)\n",
        "        X_valid2, Y_img_valid2, Y_pat_valid2, pat_valid2, path_valid2, img_valid2 = get_datasets(valid2_run, X, Y_img, Y_pat, pat, path, img)\n",
        "        X_test, Y_img_test, Y_pat_test, pat_test, path_test, img_test = get_datasets(test_run, X, Y_img, Y_pat, pat, path, img)\n",
        "        \n",
        "        \n",
        "        #### Duplicate the stroke images in train1\n",
        "        \n",
        "        # get the indices corresponding to the stroke images\n",
        "        idx = np.where(np.array(Y_img_train1) == 1)[0]\n",
        "        \n",
        "        # attach the stroke images\n",
        "        print(\"train1 before stroke duplication: \", len(X_train1), len(Y_img_train1), len(Y_pat_train1), len(pat_train1), len(path_train1), len(img_train1))\n",
        "        X_train1 = np.concatenate((X_train1, X_train1[idx]), axis=0)\n",
        "        Y_img_train1 = np.concatenate((Y_img_train1, Y_img_train1[idx]), axis=0)\n",
        "        Y_pat_train1 = np.concatenate((Y_pat_train1, Y_pat_train1[idx]), axis=0)\n",
        "        pat_train1 = np.concatenate((pat_train1, pat_train1[idx]), axis=0)\n",
        "        path_train1 = np.concatenate((path_train1, path_train1[idx]), axis=0)\n",
        "        img_train1 = np.concatenate((img_train1, img_train1[idx]), axis=0)\n",
        "        print(\"train1 after stroke duplication: \", len(X_train1), len(Y_img_train1), len(Y_pat_train1), len(pat_train1), len(path_train1), len(img_train1))\n",
        "        \n",
        "        \n",
        "        #### One hot encoding\n",
        "        \n",
        "        print('One hot encoding')\n",
        "        Y_img_train1 = convertToOneHot(Y_img_train1.astype(int), 2)\n",
        "        Y_img_valid1 = convertToOneHot(Y_img_valid1.astype(int), 2)\n",
        "        Y_img_train2 = convertToOneHot(Y_img_train2.astype(int), 2)\n",
        "        Y_img_valid2 = convertToOneHot(Y_img_valid2.astype(int), 2)\n",
        "        Y_img_test = convertToOneHot(Y_img_test.astype(int), 2)\n",
        "        \n",
        "        \n",
        "        #### Train the model\n",
        "        \n",
        "        print('#### Training')\n",
        "        \n",
        "        # generate an output folder which contains the output of the current run\n",
        "        print('Generate an output folder')\n",
        "        output_folder_tmp = output_folder + '/run' + str(i)\n",
        "        if not exists(output_folder_tmp):\n",
        "            os.makedirs(output_folder_tmp)\n",
        "            os.makedirs(output_folder_tmp + '/checkpoints')\n",
        "        \n",
        "        # start a new session\n",
        "        # print('Start new session')\n",
        "        # tf.reset_default_graph()\n",
        "        # sess = tf.Session()\n",
        "        # K.set_session(sess)\n",
        "        \n",
        "        # load the model\n",
        "        print('Load model and compile')\n",
        "        # Define hyperparameters\n",
        "        input_shape = (X_train1.shape[1], X_train1.shape[2], X_train1.shape[3])\n",
        "        \n",
        "        # load the model and compile\n",
        "        model = cnn_all_dropout(input_shape)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "        \n",
        "        # Data augmentation\n",
        "        print('Data Augmentation')\n",
        "        datagen = ImageDataGenerator(width_shift_range=0.2, \n",
        "                                     height_shift_range=0.2, \n",
        "                                     rotation_range=20, \n",
        "                                     zoom_range=0.5, \n",
        "                                     shear_range=0.2,\n",
        "                                     vertical_flip=True)\n",
        "        datagen.fit(X_train1, seed=3004)\n",
        "        \n",
        "        # tb_callback = TensorBoard(log_dir=output_folder_tmp + '/tb_output', histogram_freq=0, write_graph=True, write_images=True)\n",
        "        cp_callback = ModelCheckpoint(output_folder_tmp + '/checkpoints/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "        \n",
        "        print('Start training')\n",
        "        results = model.fit_generator(datagen.flow(X_train1, Y_img_train1, batch_size=batch_size, shuffle = True), \n",
        "                                      steps_per_epoch=len(X_train1)/batch_size, \n",
        "                                      epochs=n_epochs, \n",
        "                                      callbacks=[cp_callback], # tb_callback], \n",
        "                                      validation_data=(X_valid1, Y_img_valid1))\n",
        "        \n",
        "        # save the loss, acc etc. in a csv\n",
        "        pd.DataFrame(results.history).to_csv(output_folder_tmp + '/history.csv', index=False)\n",
        "            \n",
        "        \n",
        "        #### Predictions\n",
        "        \n",
        "        print('#### Predicting')\n",
        "\n",
        "        # Load the history file and find the model with the lowest validation loss\n",
        "        dat = pd.DataFrame.from_csv(output_folder_tmp + '/history.csv', index_col=None)\n",
        "        \n",
        "        best_epoch = str(np.where(dat.val_loss == np.min(dat.val_loss))[0][0] +1)\n",
        "        if int(best_epoch) < 10 :\n",
        "            best_epoch = \"0\" + best_epoch\n",
        "        else:\n",
        "            best_epoch = best_epoch\n",
        "            \n",
        "        mod = 'model-' + best_epoch\n",
        "        \n",
        "        # Apply the function to the differnt sets\n",
        "        print('predict train1')\n",
        "        get_predictions_cnn_all_dropout(X_train1, Y_img_train1, output_folder_tmp, 'train1', mod, \n",
        "                                        pat_train1, img_train1, Y_pat_train1, path_train1)\n",
        "        print('predict valid1')\n",
        "        get_predictions_cnn_all_dropout(X_valid1, Y_img_valid1, output_folder_tmp, 'valid1', mod, \n",
        "                                        pat_valid1, img_valid1, Y_pat_valid1, path_valid1)\n",
        "        print('predict valid2')\n",
        "        get_predictions_cnn_all_dropout(X_valid2, Y_img_valid2, output_folder_tmp, 'valid2', mod, \n",
        "                                        pat_valid2, img_valid2, Y_pat_valid2, path_valid2)\n",
        "        print('predict test')\n",
        "        get_predictions_cnn_all_dropout(X_test, Y_img_test, output_folder_tmp, 'test', mod, \n",
        "                                        pat_test, img_test, Y_pat_test, path_test)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv(train1_cv, valid1_cv, train2_cv, valid2_cv, test_cv, X_norm, Y_img, Y_pat, pat, path, img, 64, 400)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}