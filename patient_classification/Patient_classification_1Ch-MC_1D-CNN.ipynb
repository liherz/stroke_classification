{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNNs for 1Ch-MC\n",
    "- Image-level predictions + corresponding uncertainties from 1Ch-MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tf.set_random_seed(3004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = 'C:/Users/hezo/Dropbox/PhD/Stroke/Stroke_classification/Analyses_Oct_2018/outputs/5fold_CV_bl_mc_dropout/'\n",
    "folder = 'C:/Users/hezo/Documents/Stroke/patient_aggregation_1D_CNN_CV_bl_mc_dropout/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 0\n",
    "train2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_train2.csv')\n",
    "valid2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_valid2.csv')\n",
    "test_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 1\n",
    "train2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_train2.csv')\n",
    "valid2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_valid2.csv')\n",
    "test_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 2\n",
    "train2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_train2.csv')\n",
    "valid2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_valid2.csv')\n",
    "test_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 3\n",
    "train2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_train2.csv')\n",
    "valid2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_valid2.csv')\n",
    "test_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 4\n",
    "train2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_train2.csv')\n",
    "valid2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_valid2.csv')\n",
    "test_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define accuracy, specificity and sensitivity\n",
    "def acc(true, pred):\n",
    "    conf_mat = confusion_matrix(true, pred)\n",
    "    return (conf_mat[0][0]+conf_mat[1][1])/np.sum(conf_mat)\n",
    "def spec(true, pred):\n",
    "    conf_mat = confusion_matrix(true, pred)\n",
    "    return conf_mat[0][0]/np.sum(conf_mat[0])\n",
    "def sens(true, pred):\n",
    "    conf_mat = confusion_matrix(true, pred)\n",
    "    return conf_mat[1][1]/np.sum(conf_mat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the data which we use for training and validation\n",
    "train = [train2_0, train2_1, train2_2, train2_3, train2_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the data which we use for validing and validation\n",
    "valid = [valid2_0, valid2_1, valid2_2, valid2_3, valid2_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the data which we use for testing and validation\n",
    "test = [test_0, test_1, test_2, test_3, test_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertToOneHot(vector, num_classes=None):\n",
    "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN: Some tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.layers import Dense, Convolution1D, Input, Activation, Flatten, Dropout, GlobalMaxPool1D, MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_results(acc_train, acc_valid, loss_train, loss_valid):\n",
    "    plt.plot(acc_train, 'blue')\n",
    "    plt.plot(acc_valid, 'cyan')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title('Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['Train', 'Valid'], loc='lower right')\n",
    "    plt.show()\n",
    "    plt.plot(loss_train, 'blue')\n",
    "    plt.plot(loss_valid, 'cyan')\n",
    "    plt.ylim(0, 2.5)\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper right')\n",
    "    plt.show()\n",
    "    print(\"Max val accuracy: \", np.max(acc_valid))\n",
    "    print(\"In epochs: \", np.where(acc_valid==np.max(acc_valid)))\n",
    "    print(\"Min val loss: \", np.min(loss_valid))\n",
    "    print('In epoch: ', np.where(loss_valid==np.min(loss_valid)))\n",
    "    return np.where(loss_valid==np.min(loss_valid))[0]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batches_from_file(dat):\n",
    "    while True:\n",
    "        for p_id_tmp in set(dat.p_id):\n",
    "            # select one patient\n",
    "            pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
    "            # take all the predictions and save them in a list\n",
    "            X = pat_tmp.mean1.values\n",
    "            X = X.reshape((1,X.shape[0],1)) # 1 = batch, X.shape[0] = n images, 1 = n features\n",
    "            Y = pat_tmp.pat_true.head(n=1)\n",
    "            Y = convertToOneHot(Y.astype(int), 2)\n",
    "            #print(X, Y)\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #### define the model\n",
    "# num_classes = 2\n",
    "# \n",
    "# model = Sequential()\n",
    "# model.add(Convolution1D(16, kernel_size=3, activation=\"relu\", batch_input_shape=(None, None, 1)))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# \n",
    "# model.summary()\n",
    "# \n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit_generator(generate_batches_from_file(train2_0),\n",
    "#                               epochs=50,\n",
    "#                               steps_per_epoch=len(np.unique(train2_0.p_id)),\n",
    "#                               verbose=1,\n",
    "#                               validation_data=generate_batches_from_file(valid2_0),\n",
    "#                               validation_steps=len(np.unique(valid2_0.p_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show_results(history.history[\"acc\"], history.history[\"val_acc\"], history.history[\"loss\"], history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #### define the model\n",
    "# num_classes = 2\n",
    "# \n",
    "# model = Sequential()\n",
    "# model.add(Convolution1D(16, kernel_size=5, activation=\"relu\", batch_input_shape=(None, None, 1)))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# \n",
    "# model.summary()\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit_generator(generate_batches_from_file(train2_0),\n",
    "#                               epochs=50,\n",
    "#                               steps_per_epoch=len(np.unique(train2_0.p_id)),\n",
    "#                               verbose=1,\n",
    "#                               validation_data=generate_batches_from_file(valid2_0),\n",
    "#                               validation_steps=len(np.unique(valid2_0.p_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show_results(history.history[\"acc\"], history.history[\"val_acc\"], history.history[\"loss\"], history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### define the model\n",
    "# num_classes = 2\n",
    "# \n",
    "# model = Sequential()\n",
    "# model.add(Convolution1D(16, kernel_size=3, activation=\"relu\", batch_input_shape=(None, None, 1)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Convolution1D(16, kernel_size=3, activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Convolution1D(16, kernel_size=3, activation=\"relu\"))\n",
    "# \n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# \n",
    "# model.summary()\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit_generator(generate_batches_from_file(train2_0),\n",
    "#                               epochs=50,\n",
    "#                               steps_per_epoch=len(np.unique(train2_0.p_id)),\n",
    "#                               verbose=1,\n",
    "#                               validation_data=generate_batches_from_file(valid2_0),\n",
    "#                               validation_steps=len(np.unique(valid2_0.p_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show_results(history.history[\"acc\"], history.history[\"val_acc\"], history.history[\"loss\"], history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Run\", i)\n",
    "    \n",
    "    # Extract information for run i\n",
    "    train_run = train[i]\n",
    "    valid_run = valid[i]\n",
    "    \n",
    "    # Define model and compile\n",
    "    num_classes = 2\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(16, kernel_size=3, activation=\"relu\", batch_input_shape=(None, None, 1)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model and save checkpoints\n",
    "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn0/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    results = model.fit_generator(generate_batches_from_file(train_run),\n",
    "                                  epochs=30,\n",
    "                                  steps_per_epoch=len(np.unique(train_run.p_id)),\n",
    "                                  verbose=1,\n",
    "                                  validation_data=generate_batches_from_file(valid_run),\n",
    "                                  validation_steps=len(np.unique(valid_run.p_id)),\n",
    "                                  callbacks=[cp_callback])\n",
    "                    \n",
    "    # save history\n",
    "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn0/history.csv', index=False)\n",
    "    \n",
    "    #### Find epoch with lowest validation loss\n",
    "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction test\n",
    "How to get the predictions?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Extract information of run i\n",
    "test_run = test[i]\n",
    "\n",
    "# load the history\n",
    "dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0/history.csv')\n",
    "epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
    "if epoch[0]<10:\n",
    "    model = load_model(folder + 'run'+str(i)+ '/nn0/model-0' + str(epoch[0]) + '.hdf5')\n",
    "else:\n",
    "    model = load_model(folder + 'run'+str(i)+ '/nn0/model-' + str(epoch[0]) + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions\n",
    "pred = model.predict_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the true labels from the generator\n",
    "dat = test_run\n",
    "Y_true = []\n",
    "p_id = []\n",
    "for p_id_tmp in set(dat.p_id):\n",
    "    # select one patient\n",
    "    pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
    "    # take all the predictions and save them in a list\n",
    "    Y = pat_tmp.pat_true.head(n=1)\n",
    "    pid = pat_tmp.p_id.head(n=1)\n",
    "    Y_true.append(Y.values[0])\n",
    "    p_id.append(pid.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(Y_true == np.argmax(pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "accuracy = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "pred_total = []\n",
    "p_id_total = []\n",
    "pred_true_total = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(\"Run\", i)\n",
    "    #### Extract information of run i\n",
    "    test_run = test[i]\n",
    "\n",
    "    # load the history\n",
    "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0/history.csv')\n",
    "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
    "    if epoch[0]<10:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn0/model-0' + str(epoch[0]) + '.hdf5')\n",
    "    else:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn0/model-' + str(epoch[0]) + '.hdf5')\n",
    "    pred_tmp = model.predict_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id)))\n",
    "    pred = np.argmax(pred_tmp, axis=1)\n",
    "    \n",
    "    # get the true labels from the generator\n",
    "    true = []\n",
    "    p_id =  []\n",
    "    for p_id_tmp in set(test_run.p_id):\n",
    "        # select one patient\n",
    "        pat_tmp = test_run.loc[test_run.p_id==p_id_tmp,:]\n",
    "        # take all the predictions and save them in a list\n",
    "        Y = pat_tmp.pat_true.head(n=1)\n",
    "        pid = pat_tmp.p_id.head(n=1)\n",
    "        true.append(Y.values[0])\n",
    "        p_id.append(pid.values[0])\n",
    "    \n",
    "    pred_total.append(pred_tmp[:,1])\n",
    "    pred_true_total.append(true)\n",
    "    p_id_total.append(p_id)\n",
    "\n",
    "    accuracy.append(acc(true,pred))\n",
    "    specificity.append(spec(true,pred))\n",
    "    sensitivity.append(sens(true,pred))\n",
    "    print('Accuracy: ', acc(true, pred))\n",
    "    print('Specificity: ', spec(true, pred))\n",
    "    print('Sensitivity: ', sens(true, pred))\n",
    "\n",
    "pred_total = np.concatenate(pred_total)\n",
    "pred_true_total = np.concatenate(pred_true_total)\n",
    "p_id_total = np.concatenate(p_id_total)\n",
    "    \n",
    "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'pat_true':pred_true_total})\n",
    "dat.to_csv(folder + '/CV_predictions_pat_test_nn0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN with MC Dropout: predictions\n",
    "Only predictions as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Activation, Flatten, Dropout, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Run\", i)\n",
    "    \n",
    "    # Extract information for run i\n",
    "    train_run = train[i]\n",
    "    valid_run = valid[i]\n",
    "    \n",
    "    # Define model and compile\n",
    "    data_input = Input(shape=(None,1))\n",
    "    # Hidden layer\n",
    "    x = Convolution1D(16, kernel_size=3)(data_input)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    #x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    #x = Convolution1D(8, kernel_size=3)(x)\n",
    "    #x = Activation(\"relu\")(x)\n",
    "    #x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    #x = Convolution1D(8, kernel_size=3)(x)\n",
    "    #x = Activation(\"relu\")(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    x = Dense(num_classes)(x)\n",
    "    out = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=data_input, outputs=out)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model and save checkpoints\n",
    "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn0_mc/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    results = model.fit_generator(generate_batches_from_file(train_run),\n",
    "                                  epochs=80,\n",
    "                                  steps_per_epoch=len(np.unique(train_run.p_id)),\n",
    "                                  verbose=1,\n",
    "                                  validation_data=generate_batches_from_file(valid_run),\n",
    "                                  validation_steps=len(np.unique(valid_run.p_id)),\n",
    "                                  callbacks=[cp_callback])\n",
    "                    \n",
    "    # save history\n",
    "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn0_mc/history.csv', index=False)\n",
    "    \n",
    "    #### Find epoch with lowest validation loss\n",
    "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "test_run = test[i]\n",
    "\n",
    "# load the history\n",
    "dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0_mc/history.csv')\n",
    "epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
    "if epoch[0]<10:\n",
    "    model = load_model(folder + 'run'+str(i)+ '/nn0_mc/model-0' + str(epoch[0]) + '.hdf5')\n",
    "else:\n",
    "    model = load_model(folder + 'run'+str(i)+ '/nn0_mc/model-' + str(epoch[0]) + '.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = np.empty([10,102,2])\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    pred[i] = model.predict_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id))) # prediction for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std0 = np.array(np.std(pred, ddof=1, axis=0))[:,0]\n",
    "std1 = np.array(np.std(pred, ddof=1, axis=0))[:,1]\n",
    "total_var = std0**2 + std1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred[pred==0]=1e-40\n",
    "n_classes = 2\n",
    "vr = (1-(np.max(np.histogram(np.argmax(pred, axis=0), bins=n_classes, range=[0,n_classes])[0])/len(pred)))\n",
    "pe_tmp =(-1)*np.sum(np.mean(pred, axis=1)*np.log(np.mean(pred, axis=1)))\n",
    "mi = (pe_tmp + np.sum(np.array([np.sum(pred[:,i]*np.log(pred[:,i]))for i in range(0,n_classes)]))/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(pred, axis=1)*np.log(np.mean(pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(total_var.shape, vr.shape, pe_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_mean = np.mean(pred,axis=0)\n",
    "pred_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(pred_mean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "accuracy = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "pred_total = []\n",
    "p_id_total = []\n",
    "pred_true_total = []\n",
    "sd1_total = []\n",
    "total_var_total = []\n",
    "vr_total = []\n",
    "pe_total = []\n",
    "mi_total = []\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(\"Run\", i)\n",
    "    #### Extract information of run i\n",
    "    test_run = test[i]\n",
    "\n",
    "    # load the history\n",
    "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0_mc/history.csv')\n",
    "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
    "    if epoch[0]<10:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn0_mc/model-0' + str(epoch[0]) + '.hdf5')\n",
    "    else:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn0_mc/model-' + str(epoch[0]) + '.hdf5')\n",
    "    \n",
    "    # get 200 predictions for each patient\n",
    "    raw_pred = np.empty([500,len(np.unique(test_run.p_id)),2])\n",
    "    for i in range(500):\n",
    "        raw_pred[i] = model.predict_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id)))\n",
    "    # get the mean over all predictions\n",
    "    pred_tmp = np.mean(raw_pred,axis=0)\n",
    "    pred = np.argmax(pred_tmp, axis=1)\n",
    "\n",
    "    # get the true labels from the generator\n",
    "    # save the variances\n",
    "    sd0_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,0]\n",
    "    sd1_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,1]\n",
    "    total_var_total.append(sd0_tmp**2 + sd1_tmp**2)\n",
    "    sd1_total.append(sd1_tmp)\n",
    "    \n",
    "    raw_pred[raw_pred==0]=1e-40\n",
    "    j=0\n",
    "    \n",
    "    true = []\n",
    "    p_id =  []\n",
    "    vr = []\n",
    "    pe = []\n",
    "    mi = []\n",
    "    for p_id_tmp in set(test_run.p_id):\n",
    "        # select one patient\n",
    "        pat_tmp = test_run.loc[test_run.p_id==p_id_tmp,:]\n",
    "        # take all the predictions and save them in a list\n",
    "        Y = pat_tmp.pat_true.head(n=1)\n",
    "        pid = pat_tmp.p_id.head(n=1)\n",
    "        true.append(Y.values[0])\n",
    "        p_id.append(pid.values[0])\n",
    "        \n",
    "        vr.append(1-(np.max(np.histogram(np.argmax(raw_pred[:,j,:], axis=1), bins=n_classes, range=[0,n_classes])[0])/len(raw_pred[:,j,:])))\n",
    "        pe_tmp = (-1)*np.sum(np.mean(raw_pred[:,j,:], axis=0)*np.log(np.mean(raw_pred[:,j,:], axis=0)))\n",
    "        pe.append(pe_tmp)\n",
    "        mi.append(pe_tmp + np.sum(np.array([np.sum(raw_pred[:,j,i]*np.log(raw_pred[:,j,i]))for i in range(0,n_classes)]))/len(raw_pred[:,j,:]))\n",
    "        j = j+1\n",
    "        \n",
    "    \n",
    "    pred_total.append(pred_tmp[:,1])\n",
    "    pred_true_total.append(true)\n",
    "    p_id_total.append(p_id)\n",
    "    vr_total.append(vr)\n",
    "    pe_total.append(pe)\n",
    "    mi_total.append(mi)\n",
    "    \n",
    "    accuracy.append(acc(true,pred))\n",
    "    specificity.append(spec(true,pred))\n",
    "    sensitivity.append(sens(true,pred))\n",
    "    print('Accuracy: ', acc(true, pred))\n",
    "    print('Specificity: ', spec(true, pred))\n",
    "    print('Sensitivity: ', sens(true, pred))\n",
    "\n",
    "pred_total = np.concatenate(pred_total)\n",
    "pred_true_total = np.concatenate(pred_true_total)\n",
    "p_id_total = np.concatenate(p_id_total)\n",
    "sd1_total = np.concatenate(sd1_total)\n",
    "total_var_total = np.concatenate(total_var_total)\n",
    "vr_total = np.concatenate(vr_total)\n",
    "pe_total = np.concatenate(pe_total)\n",
    "mi_total = np.concatenate(mi_total)\n",
    "    \n",
    "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'pat_true':pred_true_total,\n",
    "                   'total_var':total_var_total, 'sd1':sd1_total, 'vr':vr_total, 'pe':pe_total,\n",
    "                   'mi':mi_total})\n",
    "dat.to_csv(folder + '/CV_predictions_pat_test_nn0_mc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN with MC Dropout: predictions + uncertainty measures\n",
    "Take mean prediction (mean across T softmax predictions) + uncertainty measures (variance, variation ratio, predictive entropy, mutual information) as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = train[0]\n",
    "pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
    "# take all the predictions and save them in a list + the uncertainties\n",
    "n = len(pat_tmp.mean1)\n",
    "X = np.empty((1,n,5)) # 1 = batch, X.shape[0] = n images, 1 = n features\n",
    "X[:,:,0] = pat_tmp.mean1.values\n",
    "X[:,:,1] = pat_tmp.pe.values\n",
    "X[:,:,2] = pat_tmp.vr.values\n",
    "X[:,:,3] = pat_tmp.mi.values\n",
    "X[:,:,4] = pat_tmp.total_var.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHANGE HERE\n",
    "def generate_batches_from_file_uncertainty(dat):\n",
    "    while True:\n",
    "        for p_id_tmp in set(dat.p_id):\n",
    "            # select one patient\n",
    "            pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
    "            # take all the predictions and save them in a list + the uncertainties\n",
    "            n = len(pat_tmp.mean1)\n",
    "            X = np.empty((1,n,5)) # 1 = batch, X.shape[0] = n images, 1 = n features\n",
    "            X[:,:,0] = pat_tmp.mean1.values\n",
    "            X[:,:,1] = pat_tmp.pe.values\n",
    "            X[:,:,2] = pat_tmp.vr.values\n",
    "            X[:,:,3] = pat_tmp.mi.values\n",
    "            X[:,:,4] = pat_tmp.total_var.values\n",
    "            Y = pat_tmp.pat_true.head(n=1)\n",
    "            Y = convertToOneHot(Y.astype(int), 2)\n",
    "            #print(X, Y)\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Run\", i)\n",
    "    \n",
    "    # Extract information for run i\n",
    "    train_run = train[i]\n",
    "    valid_run = valid[i]\n",
    "    \n",
    "    # Define model and compile\n",
    "    data_input = Input(shape=(None,5))\n",
    "    # Hidden layer\n",
    "    x = Convolution1D(16, kernel_size=3)(data_input)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    # x = Convolution1D(8, kernel_size=3)(x)\n",
    "    # x = Activation(\"relu\")(x)\n",
    "    # x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    # x = Convolution1D(8, kernel_size=3)(x)\n",
    "    # x = Activation(\"relu\")(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    x = Dense(num_classes)(x)\n",
    "    out = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=data_input, outputs=out)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model and save checkpoints\n",
    "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn1_mc/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    results = model.fit_generator(generate_batches_from_file_uncertainty(train_run),\n",
    "                                  epochs=100,\n",
    "                                  steps_per_epoch=len(np.unique(train_run.p_id)),\n",
    "                                  verbose=1,\n",
    "                                  validation_data=generate_batches_from_file_uncertainty(valid_run),\n",
    "                                  validation_steps=len(np.unique(valid_run.p_id)),\n",
    "                                  callbacks=[cp_callback])\n",
    "                    \n",
    "    # save history\n",
    "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn1_mc/history.csv', index=False)\n",
    "    \n",
    "    #### Find epoch with lowest validation loss\n",
    "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "accuracy = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "pred_total = []\n",
    "p_id_total = []\n",
    "pred_true_total = []\n",
    "sd1_total = []\n",
    "total_var_total = []\n",
    "vr_total = []\n",
    "pe_total = []\n",
    "mi_total = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(\"Run\", i)\n",
    "    #### Extract information of run i\n",
    "    test_run = test[i]\n",
    "\n",
    "    # load the history\n",
    "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn1_mc/history.csv')\n",
    "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
    "    if epoch[0]<10:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn1_mc/model-0' + str(epoch[0]) + '.hdf5')\n",
    "    else:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn1_mc/model-' + str(epoch[0]) + '.hdf5')\n",
    "    \n",
    "    # get 200 predictions for each patient\n",
    "    raw_pred = np.empty([500,len(np.unique(test_run.p_id)),2])\n",
    "    for i in range(500):\n",
    "        raw_pred[i] = model.predict_generator(generate_batches_from_file_uncertainty(test_run), steps=len(np.unique(test_run.p_id)))\n",
    "    # get the mean over all predictions\n",
    "    pred_tmp = np.mean(raw_pred,axis=0)\n",
    "    pred = np.argmax(pred_tmp, axis=1)\n",
    "\n",
    "    # get the true labels from the generator\n",
    "    # save the variances\n",
    "    sd0_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,0]\n",
    "    sd1_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,1]\n",
    "    total_var_total.append(sd0_tmp**2 + sd1_tmp**2)\n",
    "    sd1_total.append(sd1_tmp)\n",
    "    \n",
    "    raw_pred[raw_pred==0]=1e-40\n",
    "    j=0\n",
    "    \n",
    "    true = []\n",
    "    p_id =  []\n",
    "    vr = []\n",
    "    pe = []\n",
    "    mi = []\n",
    "    for p_id_tmp in set(test_run.p_id):\n",
    "        # select one patient\n",
    "        pat_tmp = test_run.loc[test_run.p_id==p_id_tmp,:]\n",
    "        # take all the predictions and save them in a list\n",
    "        Y = pat_tmp.pat_true.head(n=1)\n",
    "        pid = pat_tmp.p_id.head(n=1)\n",
    "        true.append(Y.values[0])\n",
    "        p_id.append(pid.values[0])\n",
    "        \n",
    "        vr.append(1-(np.max(np.histogram(np.argmax(raw_pred[:,j,:], axis=1), bins=n_classes, range=[0,n_classes])[0])/len(raw_pred[:,j,:])))\n",
    "        pe_tmp = (-1)*np.sum(np.mean(raw_pred[:,j,:], axis=0)*np.log(np.mean(raw_pred[:,j,:], axis=0)))\n",
    "        pe.append(pe_tmp)\n",
    "        mi.append(pe_tmp + np.sum(np.array([np.sum(raw_pred[:,j,i]*np.log(raw_pred[:,j,i]))for i in range(0,n_classes)]))/len(raw_pred[:,j,:]))\n",
    "        j = j+1\n",
    "        \n",
    "    \n",
    "    pred_total.append(pred_tmp[:,1])\n",
    "    pred_true_total.append(true)\n",
    "    p_id_total.append(p_id)\n",
    "    vr_total.append(vr)\n",
    "    pe_total.append(pe)\n",
    "    mi_total.append(mi)\n",
    "    \n",
    "    accuracy.append(acc(true,pred))\n",
    "    specificity.append(spec(true,pred))\n",
    "    sensitivity.append(sens(true,pred))\n",
    "    print('Accuracy: ', acc(true, pred))\n",
    "    print('Specificity: ', spec(true, pred))\n",
    "    print('Sensitivity: ', sens(true, pred))\n",
    "\n",
    "pred_total = np.concatenate(pred_total)\n",
    "pred_true_total = np.concatenate(pred_true_total)\n",
    "p_id_total = np.concatenate(p_id_total)\n",
    "sd1_total = np.concatenate(sd1_total)\n",
    "total_var_total = np.concatenate(total_var_total)\n",
    "vr_total = np.concatenate(vr_total)\n",
    "pe_total = np.concatenate(pe_total)\n",
    "mi_total = np.concatenate(mi_total)\n",
    "    \n",
    "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'pat_true':pred_true_total,\n",
    "                   'total_var':total_var_total, 'sd1':sd1_total, 'vr':vr_total, 'pe':pe_total,\n",
    "                   'mi':mi_total})\n",
    "dat.to_csv(folder + '/CV_predictions_pat_test_nn1_mc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-CNN with MC Dropout: predictive distribution\n",
    "Take histogram counts of the softmax predictions as input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 0\n",
    "train2_0 = pd.read_csv(data_folder + 'run0/raw_predictions_train2_pred1.csv')\n",
    "valid2_0 = pd.read_csv(data_folder + 'run0/raw_predictions_valid2_pred1.csv')\n",
    "test_0 = pd.read_csv(data_folder + 'run0/raw_predictions_test_pred1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 1\n",
    "train2_1 = pd.read_csv(data_folder + 'run1/raw_predictions_train2_pred1.csv')\n",
    "valid2_1 = pd.read_csv(data_folder + 'run1/raw_predictions_valid2_pred1.csv')\n",
    "test_1 = pd.read_csv(data_folder + 'run1/raw_predictions_test_pred1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 2\n",
    "train2_2 = pd.read_csv(data_folder + 'run2/raw_predictions_train2_pred1.csv')\n",
    "valid2_2 = pd.read_csv(data_folder + 'run2/raw_predictions_valid2_pred1.csv')\n",
    "test_2 = pd.read_csv(data_folder + 'run2/raw_predictions_test_pred1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 3\n",
    "train2_3 = pd.read_csv(data_folder + 'run3/raw_predictions_train2_pred1.csv')\n",
    "valid2_3 = pd.read_csv(data_folder + 'run3/raw_predictions_valid2_pred1.csv')\n",
    "test_3 = pd.read_csv(data_folder + 'run3/raw_predictions_test_pred1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run 4\n",
    "train2_4 = pd.read_csv(data_folder + 'run4/raw_predictions_train2_pred1.csv')\n",
    "valid2_4 = pd.read_csv(data_folder + 'run4/raw_predictions_valid2_pred1.csv')\n",
    "test_4 = pd.read_csv(data_folder + 'run4/raw_predictions_test_pred1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the data which we use for training and validation\n",
    "train = [train2_0, train2_1, train2_2, train2_3, train2_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the data which we use for validing and validation\n",
    "valid = [valid2_0, valid2_1, valid2_2, valid2_3, valid2_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summarize the data which we use for testing and validation\n",
    "test = [test_0, test_1, test_2, test_3, test_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # CHANGE HERE\n",
    "# def generate_batches_from_file_uncertainty(dat):\n",
    "#     while True:\n",
    "#         for p_id_tmp in set(dat.p_id):\n",
    "#             # select one patient\n",
    "#             pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
    "#             # take all the predictions and save them in a list + the uncertainties\n",
    "#             n = len(pat_tmp)\n",
    "#             X = np.empty((1,n,500)) # 1 = batch, X.shape[0] = n images, 500 = n features\n",
    "#             X[0,:,:] = pat_tmp.values[:,:500]\n",
    "#             Y = pat_tmp.pat_true.head(n=1)\n",
    "#             Y = convertToOneHot(Y.astype(int), 2)\n",
    "#             #print(X, Y)\n",
    "#             yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHANGE HERE\n",
    "def generate_batches_from_file_uncertainty(dat):\n",
    "    while True:\n",
    "        for p_id_tmp in set(dat.p_id):\n",
    "            # select one patient\n",
    "            pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
    "            # take all the predictions and save them in a list + the uncertainties\n",
    "            n = len(pat_tmp)\n",
    "            X = np.empty((1,n,100)) # 1 = batch, X.shape[0] = n images, 500 = n features\n",
    "            # X[0,:,:] = pat_tmp.values[:,:500]\n",
    "            for j in range(n):\n",
    "                X[0,j,:] = np.histogram(pat_tmp.values[j,:500][0], bins=100, range=[0,1])[0] # the counts in each interval\n",
    "            Y = pat_tmp.pat_true.head(n=1)\n",
    "            Y = convertToOneHot(Y.astype(int), 2)\n",
    "            #print(X, Y)\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Run\", i)\n",
    "    \n",
    "    # Extract information for run i\n",
    "    train_run = train[i]\n",
    "    valid_run = valid[i]\n",
    "    num_classes = 2\n",
    "    \n",
    "    # Define model and compile\n",
    "    data_input = Input(shape=(None,100))\n",
    "    # Hidden layer\n",
    "    x = Convolution1D(16, kernel_size=3)(data_input)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    # x = Convolution1D(16, kernel_size=3)(x)\n",
    "    # x = Activation(\"relu\")(x)\n",
    "    # x = Lambda(lambda x: K.dropout(x, level=0.4))(x)\n",
    "    # x = Convolution1D(32, kernel_size=3)(x)\n",
    "    # x = Activation(\"relu\")(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
    "    x = Dense(num_classes)(x)\n",
    "    out = Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=data_input, outputs=out)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model and save checkpoints\n",
    "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn2_mc/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    results = model.fit_generator(generate_batches_from_file_uncertainty(train_run),\n",
    "                                  epochs=100,\n",
    "                                  steps_per_epoch=len(np.unique(train_run.p_id)),\n",
    "                                  verbose=1,\n",
    "                                  validation_data=generate_batches_from_file_uncertainty(valid_run),\n",
    "                                  validation_steps=len(np.unique(valid_run.p_id)),\n",
    "                                  callbacks=[cp_callback])\n",
    "                    \n",
    "    # save history\n",
    "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn2_mc/history.csv', index=False)\n",
    "    \n",
    "    #### Find epoch with lowest validation loss\n",
    "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "accuracy = []\n",
    "sensitivity = []\n",
    "specificity = []\n",
    "pred_total = []\n",
    "p_id_total = []\n",
    "pred_true_total = []\n",
    "sd1_total = []\n",
    "total_var_total = []\n",
    "vr_total = []\n",
    "pe_total = []\n",
    "mi_total = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    print(\"Run\", i)\n",
    "    #### Extract information of run i\n",
    "    test_run = test[i]\n",
    "\n",
    "    # load the history\n",
    "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn2_mc/history.csv')\n",
    "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
    "    if epoch[0]<10:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn2_mc/model-0' + str(epoch[0]) + '.hdf5')\n",
    "    else:\n",
    "        model = load_model(folder + 'run'+str(i)+ '/nn2_mc/model-' + str(epoch[0]) + '.hdf5')\n",
    "    \n",
    "    # get 200 predictions for each patient\n",
    "    raw_pred = np.empty([500,len(np.unique(test_run.p_id)),2])\n",
    "    for i in range(500):\n",
    "        raw_pred[i] = model.predict_generator(generate_batches_from_file_uncertainty(test_run), steps=len(np.unique(test_run.p_id)))\n",
    "    # get the mean over all predictions\n",
    "    pred_tmp = np.mean(raw_pred,axis=0)\n",
    "    pred = np.argmax(pred_tmp, axis=1)\n",
    "\n",
    "    # get the true labels from the generator\n",
    "    # save the variances\n",
    "    sd0_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,0]\n",
    "    sd1_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,1]\n",
    "    total_var_total.append(sd0_tmp**2 + sd1_tmp**2)\n",
    "    sd1_total.append(sd1_tmp)\n",
    "    \n",
    "    raw_pred[raw_pred==0]=1e-40\n",
    "    j=0\n",
    "    \n",
    "    true = []\n",
    "    p_id =  []\n",
    "    vr = []\n",
    "    pe = []\n",
    "    mi = []\n",
    "    for p_id_tmp in set(test_run.p_id):\n",
    "        # select one patient\n",
    "        pat_tmp = test_run.loc[test_run.p_id==p_id_tmp,:]\n",
    "        # take all the predictions and save them in a list\n",
    "        Y = pat_tmp.pat_true.head(n=1)\n",
    "        pid = pat_tmp.p_id.head(n=1)\n",
    "        true.append(Y.values[0])\n",
    "        p_id.append(pid.values[0])\n",
    "        \n",
    "        vr.append(1-(np.max(np.histogram(np.argmax(raw_pred[:,j,:], axis=1), bins=n_classes, range=[0,n_classes])[0])/len(raw_pred[:,j,:])))\n",
    "        pe_tmp = (-1)*np.sum(np.mean(raw_pred[:,j,:], axis=0)*np.log(np.mean(raw_pred[:,j,:], axis=0)))\n",
    "        pe.append(pe_tmp)\n",
    "        mi.append(pe_tmp + np.sum(np.array([np.sum(raw_pred[:,j,i]*np.log(raw_pred[:,j,i]))for i in range(0,n_classes)]))/len(raw_pred[:,j,:]))\n",
    "        j = j+1\n",
    "    \n",
    "    pred_total.append(pred_tmp[:,1])\n",
    "    pred_true_total.append(true)\n",
    "    p_id_total.append(p_id)\n",
    "    vr_total.append(vr)\n",
    "    pe_total.append(pe)\n",
    "    mi_total.append(mi)\n",
    "    \n",
    "    accuracy.append(acc(true,pred))\n",
    "    specificity.append(spec(true,pred))\n",
    "    sensitivity.append(sens(true,pred))\n",
    "    print('Accuracy: ', acc(true, pred))\n",
    "    print('Specificity: ', spec(true, pred))\n",
    "    print('Sensitivity: ', sens(true, pred))\n",
    "\n",
    "pred_total = np.concatenate(pred_total)\n",
    "pred_true_total = np.concatenate(pred_true_total)\n",
    "p_id_total = np.concatenate(p_id_total)\n",
    "sd1_total = np.concatenate(sd1_total)\n",
    "total_var_total = np.concatenate(total_var_total)\n",
    "vr_total = np.concatenate(vr_total)\n",
    "pe_total = np.concatenate(pe_total)\n",
    "mi_total = np.concatenate(mi_total)\n",
    "    \n",
    "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'pat_true':pred_true_total,\n",
    "                   'total_var':total_var_total, 'sd1':sd1_total, 'vr':vr_total, 'pe':pe_total,\n",
    "                   'mi':mi_total})\n",
    "dat.to_csv(folder + '/CV_predictions_pat_test_nn2_mc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
