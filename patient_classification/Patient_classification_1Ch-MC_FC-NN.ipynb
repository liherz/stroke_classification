{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## FC-NNs for 1Ch-MC"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "tf.set_random_seed(3004)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = 'C:/Users/hezo/Dropbox/PhD/Stroke/Stroke_classification/Analyses_Oct_2018/outputs/5fold_CV_bl_mc_dropout/'\n",
        "folder = 'C:/Users/hezo/Documents/Stroke/patient_aggregation_NN_CV_bl_mc_dropout_v2/'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#run 0\n",
        "train2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_train2.csv')\n",
        "valid2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_valid2.csv')\n",
        "test_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 1\n",
        "train2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_train2.csv')\n",
        "valid2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_valid2.csv')\n",
        "test_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 2\n",
        "train2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_train2.csv')\n",
        "valid2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_valid2.csv')\n",
        "test_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 3\n",
        "train2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_train2.csv')\n",
        "valid2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_valid2.csv')\n",
        "test_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 4\n",
        "train2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_train2.csv')\n",
        "valid2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_valid2.csv')\n",
        "test_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define accuracy, specificity and sensitivity\n",
        "def acc(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return (conf_mat[0][0]+conf_mat[1][1])/np.sum(conf_mat)\n",
        "def spec(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return conf_mat[0][0]/np.sum(conf_mat[0])\n",
        "def sens(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return conf_mat[1][1]/np.sum(conf_mat[1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consider the x images which most likely show a stroke"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes which contain the information (p, pe, vr, mi)\n",
        "# of the 5 images which most likely show a stroke\n",
        "def reshape_data(dat, n):\n",
        "    # Define image and label\n",
        "    X = np.zeros((len(set(dat.p_id)), n, 5), dtype=np.float32)\n",
        "    Y = np.zeros(len(set(dat.p_id)))\n",
        "    p_id = np.zeros(len(set(dat.p_id)))\n",
        "    img = np.empty((len(set(dat.p_id)),n), dtype=np.object)\n",
        "    j = 0\n",
        "    for p_id_tmp in set(dat.p_id):\n",
        "        # select one patient and  take the n images with highest prob\n",
        "        pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
        "        pat_tmp_sorted = pat_tmp.sort_values(by=['mean1'], ascending=False)\n",
        "        pat_tmp_sorted = pat_tmp_sorted.head(n=n)\n",
        "        img[j] = pat_tmp_sorted.img\n",
        "        X[j] = pat_tmp_sorted.loc[:,['mean1','pe','vr','mi','total_var']]\n",
        "        Y[j] = pat_tmp_sorted.pat_true.head(n=1)\n",
        "        p_id[j] = pat_tmp_sorted.p_id.head(n=1)\n",
        "        j = j+1\n",
        "    Y = Y.astype(int)\n",
        "    return X, Y, p_id, img"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 5"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_0, Y_train2_0, p_id_train2_0, img_train2_0 = reshape_data(train2_0, n_images)\n",
        "print('train2: ', X_train2_0.shape, Y_train2_0.shape, p_id_train2_0.shape, img_train2_0.shape)\n",
        "X_valid2_0, Y_valid2_0, p_id_valid2_0, img_valid2_0 = reshape_data(valid2_0, n_images)\n",
        "print('Valid2: ', X_valid2_0.shape, Y_valid2_0.shape, p_id_valid2_0.shape, img_valid2_0.shape)\n",
        "X_test_0, Y_test_0, p_id_test_0, img_test_0 = reshape_data(test_0, n_images)\n",
        "print('test: ', X_test_0.shape, Y_test_0.shape, p_id_test_0.shape, img_valid2_0.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_1, Y_train2_1, p_id_train2_1, img_train2_1 = reshape_data(train2_1, n_images)\n",
        "print('train2: ', X_train2_1.shape, Y_train2_1.shape, p_id_train2_1.shape, img_train2_1.shape)\n",
        "X_valid2_1, Y_valid2_1, p_id_valid2_1, img_valid2_1 = reshape_data(valid2_1, n_images)\n",
        "print('Valid2: ', X_valid2_1.shape, Y_valid2_1.shape, p_id_valid2_1.shape, img_valid2_1.shape)\n",
        "X_test_1, Y_test_1, p_id_test_1, img_test_1 = reshape_data(test_1, n_images)\n",
        "print('test: ', X_test_1.shape, Y_test_1.shape, p_id_test_1.shape, img_test_1.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_2, Y_train2_2, p_id_train2_2, img_train2_2 = reshape_data(train2_2, n_images)\n",
        "print('train2: ', X_train2_2.shape, Y_train2_2.shape, p_id_train2_2.shape, img_train2_2.shape)\n",
        "X_valid2_2, Y_valid2_2, p_id_valid2_2, img_valid2_2 = reshape_data(valid2_2, n_images)\n",
        "print('Valid2: ', X_valid2_2.shape, Y_valid2_2.shape, p_id_valid2_2.shape, img_valid2_2.shape)\n",
        "X_test_2, Y_test_2, p_id_test_2, img_test_2 = reshape_data(test_2, n_images)\n",
        "print('test: ', X_test_2.shape, Y_test_2.shape, p_id_test_2.shape, img_test_2.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_3, Y_train2_3, p_id_train2_3, img_train2_3 = reshape_data(train2_3, n_images)\n",
        "print('train2: ', X_train2_3.shape, Y_train2_3.shape, p_id_train2_3.shape, img_train2_3.shape)\n",
        "X_valid2_3, Y_valid2_3, p_id_valid2_3, img_valid2_3 = reshape_data(valid2_3, n_images)\n",
        "print('Valid2: ', X_valid2_3.shape, Y_valid2_3.shape, p_id_valid2_3.shape, img_valid2_3.shape)\n",
        "X_test_3, Y_test_3, p_id_test_3, img_test_3 = reshape_data(test_3, n_images)\n",
        "print('test: ', X_test_3.shape, Y_test_3.shape, p_id_test_3.shape, img_test_3.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_4, Y_train2_4, p_id_train2_4, img_train2_4 = reshape_data(train2_4, n_images)\n",
        "print('train2: ', X_train2_4.shape, Y_train2_4.shape, p_id_train2_4.shape, img_train2_4.shape)\n",
        "X_valid2_4, Y_valid2_4, p_id_valid2_4, img_valid2_4 = reshape_data(valid2_4, n_images)\n",
        "print('Valid2: ', X_valid2_4.shape, Y_valid2_4.shape, p_id_valid2_4.shape, img_valid2_4.shape)\n",
        "X_test_4, Y_test_4, p_id_test_4, img_test_4 = reshape_data(test_4, n_images)\n",
        "print('test: ', X_test_4.shape, Y_test_4.shape, p_id_test_4.shape, img_test_4.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# in test4 we have one patient more, leave this out here, makes it little more complicated to program\n",
        "X_test_4 = X_test_4[:102]\n",
        "Y_test_4 = Y_test_4[:102]\n",
        "p_id_test_4 = p_id_test_4[:102]\n",
        "img_test_4 = img_test_4[:102]\n",
        "print('test: ', X_test_4.shape, Y_test_4.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and validation\n",
        "X_train = np.empty((5,X_train2_0.shape[0],X_train2_0.shape[1],X_train2_0.shape[2]))\n",
        "X_train[0] = X_train2_0\n",
        "X_train[1] = X_train2_1\n",
        "X_train[2] = X_train2_2\n",
        "X_train[3] = X_train2_3\n",
        "X_train[4] = X_train2_4\n",
        "\n",
        "Y_train = np.empty((5,Y_train2_0.shape[0]))\n",
        "Y_train[0] = Y_train2_0\n",
        "Y_train[1] = Y_train2_1\n",
        "Y_train[2] = Y_train2_2\n",
        "Y_train[3] = Y_train2_3\n",
        "Y_train[4] = Y_train2_4\n",
        "\n",
        "p_id_train = np.empty((5,p_id_train2_0.shape[0]))\n",
        "p_id_train[0] = p_id_train2_0\n",
        "p_id_train[1] = p_id_train2_1\n",
        "p_id_train[2] = p_id_train2_2\n",
        "p_id_train[3] = p_id_train2_3\n",
        "p_id_train[4] = p_id_train2_4\n",
        "\n",
        "img_train = np.empty((5,img_train2_0.shape[0],img_train2_0.shape[1]), dtype=np.object)\n",
        "img_train[0] = img_train2_0\n",
        "img_train[1] = img_train2_1\n",
        "img_train[2] = img_train2_2\n",
        "img_train[3] = img_train2_3\n",
        "img_train[4] = img_train2_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for validing and validation\n",
        "X_valid = np.empty((5,X_valid2_0.shape[0],X_valid2_0.shape[1],X_valid2_0.shape[2]))\n",
        "X_valid[0] = X_valid2_0\n",
        "X_valid[1] = X_valid2_1\n",
        "X_valid[2] = X_valid2_2\n",
        "X_valid[3] = X_valid2_3\n",
        "X_valid[4] = X_valid2_4\n",
        "\n",
        "Y_valid = np.empty((5,Y_valid2_0.shape[0]))\n",
        "Y_valid[0] = Y_valid2_0\n",
        "Y_valid[1] = Y_valid2_1\n",
        "Y_valid[2] = Y_valid2_2\n",
        "Y_valid[3] = Y_valid2_3\n",
        "Y_valid[4] = Y_valid2_4\n",
        "\n",
        "p_id_valid = np.empty((5,p_id_valid2_0.shape[0]))\n",
        "p_id_valid[0] = p_id_valid2_0\n",
        "p_id_valid[1] = p_id_valid2_1\n",
        "p_id_valid[2] = p_id_valid2_2\n",
        "p_id_valid[3] = p_id_valid2_3\n",
        "p_id_valid[4] = p_id_valid2_4\n",
        "\n",
        "img_valid = np.empty((5,img_valid2_0.shape[0],img_valid2_0.shape[1]), dtype=np.object)\n",
        "img_valid[0] = img_valid2_0\n",
        "img_valid[1] = img_valid2_1\n",
        "img_valid[2] = img_valid2_2\n",
        "img_valid[3] = img_valid2_3\n",
        "img_valid[4] = img_valid2_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for testing and testation\n",
        "X_test = np.empty((5,X_test_0.shape[0],X_test_0.shape[1],X_test_0.shape[2]))\n",
        "X_test[0] = X_test_0\n",
        "X_test[1] = X_test_1\n",
        "X_test[2] = X_test_2\n",
        "X_test[3] = X_test_3\n",
        "X_test[4] = X_test_4\n",
        "\n",
        "Y_test = np.empty((5,Y_test_0.shape[0]))\n",
        "Y_test[0] = Y_test_0\n",
        "Y_test[1] = Y_test_1\n",
        "Y_test[2] = Y_test_2\n",
        "Y_test[3] = Y_test_3\n",
        "Y_test[4] = Y_test_4\n",
        "\n",
        "p_id_test = np.empty((5,p_id_test_0.shape[0]))\n",
        "p_id_test[0] = p_id_test_0\n",
        "p_id_test[1] = p_id_test_1\n",
        "p_id_test[2] = p_id_test_2\n",
        "p_id_test[3] = p_id_test_3\n",
        "p_id_test[4] = p_id_test_4\n",
        "\n",
        "img_test = np.empty((5,img_test_0.shape[0],img_test_0.shape[1]), dtype=np.object)\n",
        "img_test[0] = img_test_0\n",
        "img_test[1] = img_test_1\n",
        "img_test[2] = img_test_2\n",
        "img_test[3] = img_test_3\n",
        "img_test[4] = img_test_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_valid.shape, X_test.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize inputs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm = np.empty((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
        "X_valid_norm = np.empty((X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], X_valid.shape[3]))\n",
        "X_test_norm = np.empty((X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3]))\n",
        "for i in range(5):\n",
        "    X_mean = np.mean(X_train[i], axis=0)\n",
        "    X_std = np.std(X_train[i], axis=0)\n",
        "    X_train_norm[i] = (X_train[i] - X_mean)/(X_std+0.00001)\n",
        "    X_valid_norm[i] = (X_valid[i] - X_mean)/(X_std+0.00001)\n",
        "    X_test_norm[i] = (X_test[i] - X_mean)/(X_std+0.00001)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data to one hot"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToOneHot(vector, num_classes=None):\n",
        "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
        "    result[np.arange(len(vector)), vector] = 1\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_new = np.empty((5,Y_train.shape[1],2))\n",
        "Y_valid_new = np.empty((5,Y_valid.shape[1],2))\n",
        "Y_test_new = np.empty((5,Y_test.shape[1],2))\n",
        "for i in range(5):\n",
        "    Y_train_new[i] = convertToOneHot(Y_train[i].astype(int), 2)\n",
        "    Y_valid_new[i] = convertToOneHot(Y_valid[i].astype(int), 2)\n",
        "    Y_test_new[i] = convertToOneHot(Y_test[i].astype(int), 2)\n",
        "print(Y_train.shape, Y_valid.shape, Y_test.shape)\n",
        "print(Y_train_new.shape, Y_valid_new.shape, Y_test_new.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC-NN with MC Dropout: Predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, Activation, Flatten, Dropout, Lambda\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as K"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(acc_train, acc_valid, loss_train, loss_valid):\n",
        "    plt.plot(acc_train, 'blue')\n",
        "    plt.plot(acc_valid, 'cyan')\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Train', 'Valid'], loc='lower right')\n",
        "    plt.show()\n",
        "    plt.plot(loss_train, 'blue')\n",
        "    plt.plot(loss_valid, 'cyan')\n",
        "    plt.ylim(0, 2.5)\n",
        "    plt.title('Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"Max val accuracy: \", np.max(acc_valid))\n",
        "    print(\"In epochs: \", np.where(acc_valid==np.max(acc_valid)))\n",
        "    print(\"Min val loss: \", np.min(loss_valid))\n",
        "    print('In epoch: ', np.where(loss_valid==np.min(loss_valid)))\n",
        "    return np.where(loss_valid==np.min(loss_valid))[0]+1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Run\", i)\n",
        "    \n",
        "    #### Extract information of run i\n",
        "    X_train_run = X_train_norm[i]\n",
        "    Y_train_run = Y_train_new[i]\n",
        "    p_id_train_run = p_id_train[i]\n",
        "    X_valid_run = X_valid_norm[i]\n",
        "    Y_valid_run = Y_valid_new[i]\n",
        "    p_id_valid_run = p_id_valid[i]\n",
        "    \n",
        "    #### define the model\n",
        "    # Inputs: p\n",
        "    data_input = Input(shape=(5,))\n",
        "    # Hidden layer\n",
        "    x = Dense(8)(data_input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    # output\n",
        "    out = Dense(2, activation='softmax', name='output')(x)\n",
        "    model = Model(inputs=data_input, outputs=out)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    #### Consider only the predictions\n",
        "    X_train_pred = X_train_run[:,:,0]\n",
        "    X_valid_pred = X_valid_run[:,:,0]\n",
        "    print(X_train_pred.shape, X_valid_pred.shape)\n",
        "    \n",
        "    #### Train the model and save checkpoints\n",
        "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn0/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "    results = model.fit(X_train_pred, Y_train_run,\n",
        "                    batch_size=2, \n",
        "                    epochs=200, \n",
        "                    validation_data=(X_valid_pred, Y_valid_run),\n",
        "                    callbacks=[cp_callback])\n",
        "    # save history\n",
        "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn0/history.csv', index=False)\n",
        "    \n",
        "    #### Find epoch with lowest validation loss\n",
        "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction: Test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the results on the test\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "mean1_total = []\n",
        "pred_total = []\n",
        "p_id_total = []\n",
        "pred_true_total = []\n",
        "sd1_total = []\n",
        "total_var_total = []\n",
        "vr_total = []\n",
        "pe_total = []\n",
        "mi_total = []  \n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    print(\"Run\", i)\n",
        "    #### Extract information of run i\n",
        "    X_test_run = X_test_norm[i]\n",
        "    Y_test_run = Y_test_new[i]\n",
        "    p_id_test_run = p_id_test[i]\n",
        "    \n",
        "    X_test_pred = X_test_run[:,:,0]\n",
        "    print(X_test_pred.shape)\n",
        "    \n",
        "    #### Start a new session\n",
        "    print('Start new session for predictions')\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    K.set_session(sess)\n",
        "    \n",
        "    \n",
        "    # load the history\n",
        "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0/history.csv')\n",
        "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
        "    if epoch[0]<10:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn0/model-0' + str(epoch[0]) + '.hdf5')\n",
        "    else:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn0/model-' + str(epoch[0]) + '.hdf5')\n",
        "    # pred = np.argmax(model.predict(X_test_pred), axis=1)\n",
        "    # true = np.argmax(Y_test_run, axis=1)\n",
        "    \n",
        "    print('Get the predictions')\n",
        "    n_classes = 2\n",
        "    \n",
        "    predictions = np.zeros((len(X_test_pred), 500, n_classes))\n",
        "    mean1 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd0 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd1 = np.zeros((X_test_pred.shape[0]))\n",
        "    total_var = np.zeros((X_test_pred.shape[0]))\n",
        "    vr = []\n",
        "    pe = []\n",
        "    mi = [] \n",
        "    for j in range(len(X_test_pred)):\n",
        "        # repeat the current image 500 times\n",
        "        X_rep = np.empty((500,X_test_pred.shape[1]))\n",
        "        X_rep[:] = X_test_pred[j:j+1]\n",
        "        # get 500 predictions for this image\n",
        "        pred = sess.run(model2.output, feed_dict={model2.input: X_rep})\n",
        "        \n",
        "        # output of mean and sd == #classes\n",
        "        predictions[j] = pred # save the raw predictions\n",
        "        mean1[j] = np.mean(pred, axis=0)[1]\n",
        "        sd0[j], sd1[j] = np.array(np.std(pred, ddof=1, axis=0))\n",
        "        total_var[j] = sd0[j]**2 + sd1[j]**2\n",
        "        pred[pred==0]=1e-40\n",
        "        vr.append(1-(np.max(np.histogram(np.argmax(pred, axis=1), bins=n_classes, range=[0,n_classes])[0])/len(pred)))\n",
        "        pe_tmp = (-1)*np.sum(np.mean(pred, axis=0)*np.log(np.mean(pred, axis=0)))\n",
        "        pe.append(pe_tmp)\n",
        "        mi.append(pe_tmp + np.sum(np.array([np.sum(pred[:,i]*np.log(pred[:,i]))for i in range(0,n_classes)]))/len(pred))\n",
        "    \n",
        "    true = np.argmax(Y_test_run,axis=1)\n",
        "    pred = np.round(mean1)\n",
        "    \n",
        "    mean1_total.append(mean1)\n",
        "    pred_total.append(pred)\n",
        "    pred_true_total.append(true)\n",
        "    p_id_total.append(p_id_test_run)\n",
        "    sd1_total.append(sd1)\n",
        "    total_var_total.append(total_var)\n",
        "    vr_total.append(vr)\n",
        "    pe_total.append(pe)\n",
        "    mi_total.append(mi)\n",
        "\n",
        "    accuracy.append(acc(true,pred))\n",
        "    specificity.append(spec(true,pred))\n",
        "    sensitivity.append(sens(true,pred))\n",
        "    print('Accuracy: ', acc(true, pred))\n",
        "    print('Specificity: ', spec(true, pred))\n",
        "    print('Sensitivity: ', sens(true, pred))\n",
        "\n",
        "mean1_total = np.concatenate(mean1_total)\n",
        "pred_total = np.concatenate(pred_total)\n",
        "pred_true_total = np.concatenate(pred_true_total)\n",
        "p_id_total = np.concatenate(p_id_total)\n",
        "sd1_total = np.concatenate(sd1_total)\n",
        "total_var_total = np.concatenate(total_var_total)\n",
        "vr_total = np.concatenate(vr_total)\n",
        "pe_total = np.concatenate(pe_total)\n",
        "mi_total = np.concatenate(mi_total)\n",
        "    \n",
        "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'mean_pred':mean1_total, 'pat_true':pred_true_total,\n",
        "                   'sd':sd1_total, 'total_var':total_var_total, 'vr':vr_total, \n",
        "                    'pe':pe_total, 'mi':mi_total})\n",
        "dat.to_csv(folder + '/CV_predictions_pat_test_nn0_mc_dropout.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC-NN with MC Dropout: Predictions + uncertainty measures\n",
        "Take mean prediction (mean across T softmax predictions) + uncertainty measures (variance, variation ratio, predictive entropy, mutual information) as input"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Parallel Network\n",
        "from keras.layers import Dense, concatenate, Lambda, Reshape, Input, Activation, Flatten\n",
        "from keras.models import Model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Run\", i)\n",
        "    \n",
        "    #### Extract information of run i\n",
        "    X_train_run = X_train_norm[i]\n",
        "    Y_train_run = Y_train_new[i]\n",
        "    p_id_train_run = p_id_train[i]\n",
        "    X_valid_run = X_valid_norm[i]\n",
        "    Y_valid_run = Y_valid_new[i]\n",
        "    p_id_valid_run = p_id_valid[i]\n",
        "    \n",
        "    #### define the model\n",
        "    # Inputs: (p, pe, mi, vr, var),  --> 5 measures\n",
        "    data_input = Input(shape=(5,))\n",
        "    # Hidden layer\n",
        "    x = Dense(8)(data_input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.4))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.4))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    nn_out = Lambda(lambda x: K.dropout(x, level=0.4))(x)\n",
        "    model1 = Model(inputs=data_input, outputs=nn_out)\n",
        "    \n",
        "    # Read in the data ans split the images\n",
        "    img = Input(shape=(5,5), name='img_input') # (5=n images, 5=pred)\n",
        "    img1 = Lambda(lambda x: x[:,0,:])(img) # Information of image 1\n",
        "    img2 = Lambda(lambda x: x[:,1,:])(img) # Information of image 2 ...\n",
        "    img3 = Lambda(lambda x: x[:,2,:])(img)\n",
        "    img4 = Lambda(lambda x: x[:,3,:])(img) \n",
        "    img5 = Lambda(lambda x: x[:,4,:])(img)\n",
        "    # img6 = Lambda(lambda x: x[:,5,:])(img)\n",
        "    # img7 = Lambda(lambda x: x[:,6,:])(img) \n",
        "    # img8 = Lambda(lambda x: x[:,7,:])(img)    \n",
        "    # img9 = Lambda(lambda x: x[:,8,:])(img)\n",
        "    # img10 = Lambda(lambda x: x[:,9,:])(img) \n",
        "    # img11 = Lambda(lambda x: x[:,10,:])(img)    \n",
        "    # img12 = Lambda(lambda x: x[:,11,:])(img)\n",
        "    # img13 = Lambda(lambda x: x[:,12,:])(img) \n",
        "    # img14 = Lambda(lambda x: x[:,13,:])(img)\n",
        "    # img15 = Lambda(lambda x: x[:,14,:])(img)\n",
        "\n",
        "    # Aply the same model with the same weights to each image\n",
        "    # and then calculate the loss\n",
        "    img1_out = model1(img1)\n",
        "    img2_out = model1(img2)\n",
        "    img3_out = model1(img3)\n",
        "    img4_out = model1(img4)\n",
        "    img5_out = model1(img5)\n",
        "    # img6_out = model1(img6)\n",
        "    # img7_out = model1(img7)\n",
        "    # img8_out = model1(img8)\n",
        "    # img9_out = model1(img9)\n",
        "    # img10_out = model1(img10)\n",
        "    # img11_out = model1(img11)\n",
        "    # img12_out = model1(img12)\n",
        "    # img13_out = model1(img13)\n",
        "    # img14_out = model1(img14)\n",
        "    # img15_out = model1(img15)\n",
        "    # concatenate the output of the same model with the differnt images\n",
        "    concatenated = concatenate([img1_out, img2_out, img3_out, img4_out, img5_out], axis=1)\n",
        "                               # img6_out, img7_out, img8_out, img9_out, img10_out], axis=1)\n",
        "                               # img11_out, img12_out, img13_out, img14_out, img15_out], axis=1)\n",
        "    x = Dense(8)(concatenated)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.4))(x)\n",
        "    #x = Dense(4)(x)\n",
        "    #x = Activation('relu')(x)\n",
        "    #x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    out = Dense(2, activation='softmax', name='output')(x)\n",
        "    model = Model(img, out)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    #### Consider only the predictions\n",
        "    X_train_pred = X_train_run\n",
        "    X_valid_pred = X_valid_run\n",
        "    print(X_train_pred.shape, X_valid_pred.shape)\n",
        "    \n",
        "    #### Train the model and save checkpoints\n",
        "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn1/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "    results = model.fit(X_train_pred, Y_train_run,\n",
        "                    batch_size=2, \n",
        "                    epochs=200, \n",
        "                    validation_data=(X_valid_pred, Y_valid_run),\n",
        "                    callbacks=[cp_callback])\n",
        "    # save history\n",
        "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn1/history.csv', index=False)\n",
        "    \n",
        "    #### Find epoch with lowest validation loss\n",
        "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction: Test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the results on the test\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "accuracy = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "mean1_total = []\n",
        "pred_total = []\n",
        "p_id_total = []\n",
        "pred_true_total = []\n",
        "sd1_total = []\n",
        "total_var_total = []\n",
        "vr_total = []\n",
        "pe_total = []\n",
        "mi_total = []  \n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    print(\"Run\", i)\n",
        "    #### Extract information of run i\n",
        "    X_test_run = X_test_norm[i]\n",
        "    Y_test_run = Y_test_new[i]\n",
        "    p_id_test_run = p_id_test[i]\n",
        "    \n",
        "    X_test_pred = X_test_run\n",
        "    print(X_test_pred.shape)\n",
        "    \n",
        "    #### Start a new session\n",
        "    print('Start new session for predictions')\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    K.set_session(sess)\n",
        "   \n",
        "    \n",
        "    # load the history\n",
        "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn1/history.csv')\n",
        "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
        "    if epoch[0]<10:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn1/model-0' + str(epoch[0]) + '.hdf5')\n",
        "    else:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn1/model-' + str(epoch[0]) + '.hdf5')\n",
        "    # pred = np.argmax(model.predict(X_test_pred), axis=1)\n",
        "    # true = np.argmax(Y_test_run, axis=1)\n",
        "    \n",
        "    print('Get the predictions')\n",
        "    n_classes = 2\n",
        "    \n",
        "    predictions = np.zeros((len(X_test_pred), 500, n_classes))\n",
        "    mean1 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd0 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd1 = np.zeros((X_test_pred.shape[0]))\n",
        "    total_var = np.zeros((X_test_pred.shape[0]))\n",
        "    vr = []\n",
        "    pe = []\n",
        "    mi = [] \n",
        "    for j in range(len(X_test_pred)):\n",
        "        # repeat the current image 500 times\n",
        "        X_rep = np.empty((500,X_test_pred.shape[1],X_test_pred.shape[2]))\n",
        "        X_rep[:] = X_test_pred[j:j+1]\n",
        "        # get 500 predictions for this image\n",
        "        pred = sess.run(model2.output, feed_dict={model2.input: X_rep})\n",
        "        \n",
        "        # output of mean and sd == #classes\n",
        "        predictions[j] = pred # save the raw predictions\n",
        "        mean1[j] = np.mean(pred, axis=0)[1]\n",
        "        sd0[j], sd1[j] = np.array(np.std(pred, ddof=1, axis=0))\n",
        "        total_var[j] = sd0[j]**2 + sd1[j]**2\n",
        "        pred[pred==0]=1e-40\n",
        "        vr.append(1-(np.max(np.histogram(np.argmax(pred, axis=1), bins=n_classes, range=[0,n_classes])[0])/len(pred)))\n",
        "        pe_tmp = (-1)*np.sum(np.mean(pred, axis=0)*np.log(np.mean(pred, axis=0)))\n",
        "        pe.append(pe_tmp)\n",
        "        mi.append(pe_tmp + np.sum(np.array([np.sum(pred[:,i]*np.log(pred[:,i]))for i in range(0,n_classes)]))/len(pred))\n",
        "    \n",
        "    true = np.argmax(Y_test_run,axis=1)\n",
        "    pred = np.round(mean1)\n",
        "    \n",
        "    mean1_total.append(mean1)\n",
        "    pred_total.append(pred)\n",
        "    pred_true_total.append(true)\n",
        "    p_id_total.append(p_id_test_run)\n",
        "    sd1_total.append(sd1)\n",
        "    total_var_total.append(total_var)\n",
        "    vr_total.append(vr)\n",
        "    pe_total.append(pe)\n",
        "    mi_total.append(mi)\n",
        "\n",
        "    accuracy.append(acc(true,pred))\n",
        "    specificity.append(spec(true,pred))\n",
        "    sensitivity.append(sens(true,pred))\n",
        "    print('Accuracy: ', acc(true, pred))\n",
        "    print('Specificity: ', spec(true, pred))\n",
        "    print('Sensitivity: ', sens(true, pred))\n",
        "\n",
        "mean1_total = np.concatenate(mean1_total)\n",
        "pred_total = np.concatenate(pred_total)\n",
        "pred_true_total = np.concatenate(pred_true_total)\n",
        "p_id_total = np.concatenate(p_id_total)\n",
        "sd1_total = np.concatenate(sd1_total)\n",
        "total_var_total = np.concatenate(total_var_total)\n",
        "vr_total = np.concatenate(vr_total)\n",
        "pe_total = np.concatenate(pe_total)\n",
        "mi_total = np.concatenate(mi_total)\n",
        "    \n",
        "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'mean_pred':mean1_total, 'pat_true':pred_true_total,\n",
        "                   'sd':sd1_total, 'total_var':total_var_total, 'vr':vr_total, \n",
        "                    'pe':pe_total, 'mi':mi_total})\n",
        "dat.to_csv(folder + '/CV_predictions_pat_test_nn1_mc_dropout.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC-NN with MC Dropout: predictive distribution\n",
        "Take histogram counts of the softmax predictions as input to the network"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#run 0\n",
        "train2_0 = pd.read_csv(data_folder + 'run0/raw_predictions_train2_pred1.csv')\n",
        "valid2_0 = pd.read_csv(data_folder + 'run0/raw_predictions_valid2_pred1.csv')\n",
        "test_0 = pd.read_csv(data_folder + 'run0/raw_predictions_test_pred1.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 1\n",
        "train2_1 = pd.read_csv(data_folder + 'run1/raw_predictions_train2_pred1.csv')\n",
        "valid2_1 = pd.read_csv(data_folder + 'run1/raw_predictions_valid2_pred1.csv')\n",
        "test_1 = pd.read_csv(data_folder + 'run1/raw_predictions_test_pred1.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 2\n",
        "train2_2 = pd.read_csv(data_folder + 'run2/raw_predictions_train2_pred1.csv')\n",
        "valid2_2 = pd.read_csv(data_folder + 'run2/raw_predictions_valid2_pred1.csv')\n",
        "test_2 = pd.read_csv(data_folder + 'run2/raw_predictions_test_pred1.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 3\n",
        "train2_3 = pd.read_csv(data_folder + 'run3/raw_predictions_train2_pred1.csv')\n",
        "valid2_3 = pd.read_csv(data_folder + 'run3/raw_predictions_valid2_pred1.csv')\n",
        "test_3 = pd.read_csv(data_folder + 'run3/raw_predictions_test_pred1.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 4\n",
        "train2_4 = pd.read_csv(data_folder + 'run4/raw_predictions_train2_pred1.csv')\n",
        "valid2_4 = pd.read_csv(data_folder + 'run4/raw_predictions_valid2_pred1.csv')\n",
        "test_4 = pd.read_csv(data_folder + 'run4/raw_predictions_test_pred1.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and validation\n",
        "train = [train2_0, train2_1, train2_2, train2_3, train2_4]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for validing and validation\n",
        "valid = [valid2_0, valid2_1, valid2_2, valid2_3, valid2_4]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for testing and validation\n",
        "test = [test_0, test_1, test_2, test_3, test_4]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Find the x images which most likely show a stroke corresponding to the same images above"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get the mean across the values\n",
        "train[0] = train[0].assign(mean1 = train[0].iloc[:,:500].mean(axis=1).values)\n",
        "train[1] = train[1].assign(mean1 = train[1].iloc[:,:500].mean(axis=1).values)\n",
        "train[2] = train[2].assign(mean1 = train[2].iloc[:,:500].mean(axis=1).values)\n",
        "train[3] = train[3].assign(mean1 = train[3].iloc[:,:500].mean(axis=1).values)\n",
        "train[4] = train[4].assign(mean1 = train[4].iloc[:,:500].mean(axis=1).values)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the mean across the values\n",
        "valid[0] = valid[0].assign(mean1 = valid[0].iloc[:,:500].mean(axis=1).values)\n",
        "valid[1] = valid[1].assign(mean1 = valid[1].iloc[:,:500].mean(axis=1).values)\n",
        "valid[2] = valid[2].assign(mean1 = valid[2].iloc[:,:500].mean(axis=1).values)\n",
        "valid[3] = valid[3].assign(mean1 = valid[3].iloc[:,:500].mean(axis=1).values)\n",
        "valid[4] = valid[4].assign(mean1 = valid[4].iloc[:,:500].mean(axis=1).values)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the mean across the values\n",
        "test[0] = test[0].assign(mean1 = test[0].iloc[:,:500].mean(axis=1).values)\n",
        "test[1] = test[1].assign(mean1 = test[1].iloc[:,:500].mean(axis=1).values)\n",
        "test[2] = test[2].assign(mean1 = test[2].iloc[:,:500].mean(axis=1).values)\n",
        "test[3] = test[3].assign(mean1 = test[3].iloc[:,:500].mean(axis=1).values)\n",
        "test[4] = test[4].assign(mean1 = test[4].iloc[:,:500].mean(axis=1).values)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the histogram values"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes which contain the information (p, pe, vr, mi)\n",
        "# of the 5 images which most likely show a stroke\n",
        "def reshape_data2(dat, n):\n",
        "    # Define image and label\n",
        "    X = np.zeros((len(set(dat.p_id)), n, 100), dtype=np.float32)\n",
        "    Y = np.zeros(len(set(dat.p_id)))\n",
        "    p_id = np.zeros(len(set(dat.p_id)))\n",
        "    img = np.empty((len(set(dat.p_id)),n), dtype=np.object)\n",
        "    i=0\n",
        "    pids = set(dat.p_id)\n",
        "    pids = set(filter(lambda x: x == x , pids)) # to ensure that there are no NAs\n",
        "    for p_id_tmp in pids:\n",
        "        # select one patient and  take the n images with highest prob\n",
        "        pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
        "        pat_tmp_sorted = pat_tmp.sort_values(by=['mean1'], ascending=False)\n",
        "        pat_tmp_sorted = pat_tmp_sorted.head(n=n)\n",
        "        for j in range(n):\n",
        "            X[i,j,:] = np.histogram(pat_tmp_sorted.values[j,:500], bins=100, range=[0,1])[0] # the counts in each interval\n",
        "        Y[i] = pat_tmp_sorted.pat_true.head(n=1)\n",
        "        p_id[i] = pat_tmp_sorted.p_id.head(n=1)\n",
        "        img[i] = pat_tmp_sorted.img\n",
        "        i = i+1\n",
        "    Y = Y.astype(int)\n",
        "    return X, Y, p_id, img"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dat = test[0]\n",
        "# i=0\n",
        "# n=5\n",
        "# X = np.zeros((len(set(dat.p_id)), n, 100), dtype=np.float32)\n",
        "# p_id_tmp = 515\n",
        "# pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
        "# pat_tmp\n",
        "# pat_tmp_sorted = pat_tmp.sort_values(by=['mean1'], ascending=False)\n",
        "# pat_tmp_sorted = pat_tmp_sorted.head(n=n)\n",
        "# pat_tmp_sorted\n",
        "# for j in range(n):\n",
        "#     X[i,j,:] = np.histogram(pat_tmp.values[j,:500], bins=100, range=[0,1])[0] # the counts in each interval"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_0, Y_train_0, p_id_train_0, img_train_0 = reshape_data2(train[0],5)\n",
        "X_train_1, Y_train_1, p_id_train_1, img_train_1 = reshape_data2(train[1],5)\n",
        "X_train_2, Y_train_2, p_id_train_2, img_train_2 = reshape_data2(train[2],5)\n",
        "X_train_3, Y_train_3, p_id_train_3, img_train_3 = reshape_data2(train[3],5)\n",
        "X_train_4, Y_train_4, p_id_train_4, img_train_4 = reshape_data2(train[4],5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid_0, Y_valid_0, p_id_valid_0, img_valid_0 = reshape_data2(valid[0],5)\n",
        "X_valid_1, Y_valid_1, p_id_valid_1, img_valid_1 = reshape_data2(valid[1],5)\n",
        "X_valid_2, Y_valid_2, p_id_valid_2, img_valid_2 = reshape_data2(valid[2],5)\n",
        "X_valid_3, Y_valid_3, p_id_valid_3, img_valid_3 = reshape_data2(valid[3],5)\n",
        "X_valid_4, Y_valid_4, p_id_valid_4, img_valid_4 = reshape_data2(valid[4],5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_0, Y_test_0, p_id_test_0, img_test_0 = reshape_data2(test[0],5)\n",
        "X_test_1, Y_test_1, p_id_test_1, img_test_1 = reshape_data2(test[1],5)\n",
        "X_test_2, Y_test_2, p_id_test_2, img_test_2 = reshape_data2(test[2],5)\n",
        "X_test_3, Y_test_3, p_id_test_3, img_test_3 = reshape_data2(test[3],5)\n",
        "X_test_4, Y_test_4, p_id_test_4, img_test_4 = reshape_data2(test[4],5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and validation\n",
        "X_train = np.empty((5,X_train_0.shape[0],X_train_0.shape[1],X_train_0.shape[2]))\n",
        "X_train[0] = X_train_0\n",
        "X_train[1] = X_train_1\n",
        "X_train[2] = X_train_2\n",
        "X_train[3] = X_train_3\n",
        "X_train[4] = X_train_4\n",
        "\n",
        "Y_train = np.empty((5,Y_train_0.shape[0]))\n",
        "Y_train[0] = Y_train_0\n",
        "Y_train[1] = Y_train_1\n",
        "Y_train[2] = Y_train_2\n",
        "Y_train[3] = Y_train_3\n",
        "Y_train[4] = Y_train_4\n",
        "\n",
        "p_id_train = np.empty((5,p_id_train_0.shape[0]))\n",
        "p_id_train[0] = p_id_train_0\n",
        "p_id_train[1] = p_id_train_1\n",
        "p_id_train[2] = p_id_train_2\n",
        "p_id_train[3] = p_id_train_3\n",
        "p_id_train[4] = p_id_train_4\n",
        "\n",
        "img_train = np.empty((5,img_train_0.shape[0],img_train_0.shape[1]), dtype=np.object)\n",
        "img_train[0] = img_train_0\n",
        "img_train[1] = img_train_1\n",
        "img_train[2] = img_train_2\n",
        "img_train[3] = img_train_3\n",
        "img_train[4] = img_train_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for validing and validation\n",
        "X_valid = np.empty((5,X_valid_0.shape[0],X_valid_0.shape[1],X_valid_0.shape[2]))\n",
        "X_valid[0] = X_valid_0\n",
        "X_valid[1] = X_valid_1\n",
        "X_valid[2] = X_valid_2\n",
        "X_valid[3] = X_valid_3\n",
        "X_valid[4] = X_valid_4\n",
        "\n",
        "Y_valid = np.empty((5,Y_valid_0.shape[0]))\n",
        "Y_valid[0] = Y_valid_0\n",
        "Y_valid[1] = Y_valid_1\n",
        "Y_valid[2] = Y_valid_2\n",
        "Y_valid[3] = Y_valid_3\n",
        "Y_valid[4] = Y_valid_4\n",
        "\n",
        "p_id_valid = np.empty((5,p_id_valid_0.shape[0]))\n",
        "p_id_valid[0] = p_id_valid_0\n",
        "p_id_valid[1] = p_id_valid_1\n",
        "p_id_valid[2] = p_id_valid_2\n",
        "p_id_valid[3] = p_id_valid_3\n",
        "p_id_valid[4] = p_id_valid_4\n",
        "\n",
        "img_valid = np.empty((5,img_valid_0.shape[0],img_valid_0.shape[1]), dtype=np.object)\n",
        "img_valid[0] = img_valid_0\n",
        "img_valid[1] = img_valid_1\n",
        "img_valid[2] = img_valid_2\n",
        "img_valid[3] = img_valid_3\n",
        "img_valid[4] = img_valid_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for testing and validation\n",
        "X_test = np.empty((5,X_test_0.shape[0],X_test_0.shape[1],X_test_0.shape[2]))\n",
        "X_test[0] = X_test_0\n",
        "X_test[1] = X_test_1\n",
        "X_test[2] = X_test_2\n",
        "X_test[3] = X_test_3\n",
        "X_test[4] = X_test_4[:102]\n",
        "\n",
        "Y_test = np.empty((5,Y_test_0.shape[0]))\n",
        "Y_test[0] = Y_test_0\n",
        "Y_test[1] = Y_test_1\n",
        "Y_test[2] = Y_test_2\n",
        "Y_test[3] = Y_test_3\n",
        "Y_test[4] = Y_test_4[:102]\n",
        "\n",
        "p_id_test = np.empty((5,p_id_test_0.shape[0]))\n",
        "p_id_test[0] = p_id_test_0\n",
        "p_id_test[1] = p_id_test_1\n",
        "p_id_test[2] = p_id_test_2\n",
        "p_id_test[3] = p_id_test_3\n",
        "p_id_test[4] = p_id_test_4[:102]\n",
        "\n",
        "img_test = np.empty((5,img_test_0.shape[0],img_test_0.shape[1]), dtype=np.object)\n",
        "img_test[0] = img_test_0\n",
        "img_test[1] = img_test_1\n",
        "img_test[2] = img_test_2\n",
        "img_test[3] = img_test_3\n",
        "img_test[4] = img_test_4[:102]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalize"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm = np.empty((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
        "X_valid_norm = np.empty((X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], X_valid.shape[3]))\n",
        "X_test_norm = np.empty((X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3]))\n",
        "for i in range(5):\n",
        "    X_mean = np.mean(X_train[i], axis=0)\n",
        "    X_std = np.std(X_train[i], axis=0)\n",
        "    X_train_norm[i] = (X_train[i] - X_mean)/(X_std+0.00001)\n",
        "    X_valid_norm[i] = (X_valid[i] - X_mean)/(X_std+0.00001)\n",
        "    X_test_norm[i] = (X_test[i] - X_mean)/(X_std+0.00001)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data to one hot"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_new = np.empty((5,Y_train.shape[1],2))\n",
        "Y_valid_new = np.empty((5,Y_valid.shape[1],2))\n",
        "Y_test_new = np.empty((5,Y_test.shape[1],2))\n",
        "for i in range(5):\n",
        "    Y_train_new[i] = convertToOneHot(Y_train[i].astype(int), 2)\n",
        "    Y_valid_new[i] = convertToOneHot(Y_valid[i].astype(int), 2)\n",
        "    Y_test_new[i] = convertToOneHot(Y_test[i].astype(int), 2)\n",
        "print(Y_train.shape, Y_valid.shape, Y_test.shape)\n",
        "print(Y_train_new.shape, Y_valid_new.shape, Y_test_new.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "### Parallel Network\n",
        "from keras.layers import Dense, concatenate, Lambda, Reshape, Input, Activation, Flatten\n",
        "from keras.models import Model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n00 = X_train_norm[0]\n",
        "n00.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Run\", i)\n",
        "    \n",
        "    #### Extract information of run i\n",
        "    X_train_run = X_train_norm[i]\n",
        "    Y_train_run = Y_train_new[i]\n",
        "    p_id_train_run = p_id_train[i]\n",
        "    X_valid_run = X_valid_norm[i]\n",
        "    Y_valid_run = Y_valid_new[i]\n",
        "    p_id_valid_run = p_id_valid[i]\n",
        "    \n",
        "    #### define the model\n",
        "    # Inputs: histograms  --> 100 measures\n",
        "    data_input = Input(shape=(100,))\n",
        "    # Hidden layer\n",
        "    x = Dense(8)(data_input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    nn_out = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    model1 = Model(inputs=data_input, outputs=nn_out)\n",
        "    \n",
        "    # Read in the data and split the images\n",
        "    img = Input(shape=(5,100), name='img_input') # (5=n images, 5=pred)\n",
        "    img1 = Lambda(lambda x: x[:,0,:])(img) # Information of image 1\n",
        "    img2 = Lambda(lambda x: x[:,1,:])(img) # Information of image 2 ...\n",
        "    img3 = Lambda(lambda x: x[:,2,:])(img)\n",
        "    img4 = Lambda(lambda x: x[:,3,:])(img) \n",
        "    img5 = Lambda(lambda x: x[:,4,:])(img)\n",
        "\n",
        "    # Aply the same model with the same weights to each image\n",
        "    # and then calculate the loss\n",
        "    img1_out = model1(img1)\n",
        "    img2_out = model1(img2)\n",
        "    img3_out = model1(img3)\n",
        "    img4_out = model1(img4)\n",
        "    img5_out = model1(img5)\n",
        "    # concatenate the output of the same model with the differnt images\n",
        "    concatenated = concatenate([img1_out, img2_out, img3_out, img4_out, img5_out], axis=1)\n",
        "    x = Dense(8)(concatenated)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    #x = Dense(4)(x)\n",
        "    #x = Activation('relu')(x)\n",
        "    #x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    out = Dense(2, activation='softmax', name='output')(x)\n",
        "    model = Model(img, out)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    #### Consider only the predictions\n",
        "    \n",
        "    #### Train the model and save checkpoints\n",
        "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn2/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "    results = model.fit(X_train_run, Y_train_run,\n",
        "                    batch_size=2, \n",
        "                    epochs=200, \n",
        "                    validation_data=(X_valid_run, Y_valid_run),\n",
        "                    callbacks=[cp_callback])\n",
        "    # save history\n",
        "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn2/history.csv', index=False)\n",
        "    \n",
        "    #### Find epoch with lowest validation loss\n",
        "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction: Test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the results on the test\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "accuracy = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "mean1_total = []\n",
        "pred_total = []\n",
        "p_id_total = []\n",
        "pred_true_total = []\n",
        "sd1_total = []\n",
        "total_var_total = []\n",
        "vr_total = []\n",
        "pe_total = []\n",
        "mi_total = []  \n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    print(\"Run\", i)\n",
        "    #### Extract information of run i\n",
        "    X_test_run = X_test_norm[i]\n",
        "    Y_test_run = Y_test_new[i]\n",
        "    p_id_test_run = p_id_test[i]\n",
        "    \n",
        "    X_test_pred = X_test_run\n",
        "    print(X_test_pred.shape)\n",
        "    \n",
        "    #### Start a new session\n",
        "    print('Start new session for predictions')\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    K.set_session(sess)\n",
        "   \n",
        "    \n",
        "    # load the history\n",
        "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn2/history.csv')\n",
        "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
        "    if epoch[0]<10:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn2/model-0' + str(epoch[0]) + '.hdf5')\n",
        "    else:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn2/model-' + str(epoch[0]) + '.hdf5')\n",
        "    # pred = np.argmax(model.predict(X_test_pred), axis=1)\n",
        "    # true = np.argmax(Y_test_run, axis=1)\n",
        "    \n",
        "    print('Get the predictions')\n",
        "    n_classes = 2\n",
        "    \n",
        "    predictions = np.zeros((len(X_test_pred), 500, n_classes))\n",
        "    mean1 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd0 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd1 = np.zeros((X_test_pred.shape[0]))\n",
        "    total_var = np.zeros((X_test_pred.shape[0]))\n",
        "    vr = []\n",
        "    pe = []\n",
        "    mi = [] \n",
        "    for j in range(len(X_test_pred)):\n",
        "        # repeat the current image 500 times\n",
        "        X_rep = np.empty((500,X_test_pred.shape[1],X_test_pred.shape[2]))\n",
        "        X_rep[:] = X_test_pred[j:j+1]\n",
        "        # get 500 predictions for this image\n",
        "        pred = sess.run(model2.output, feed_dict={model2.input: X_rep})\n",
        "        \n",
        "        # output of mean and sd == #classes\n",
        "        predictions[j] = pred # save the raw predictions\n",
        "        mean1[j] = np.mean(pred, axis=0)[1]\n",
        "        sd0[j], sd1[j] = np.array(np.std(pred, ddof=1, axis=0))\n",
        "        total_var[j] = sd0[j]**2 + sd1[j]**2\n",
        "        pred[pred==0]=1e-40\n",
        "        vr.append(1-(np.max(np.histogram(np.argmax(pred, axis=1), bins=n_classes, range=[0,n_classes])[0])/len(pred)))\n",
        "        pe_tmp = (-1)*np.sum(np.mean(pred, axis=0)*np.log(np.mean(pred, axis=0)))\n",
        "        pe.append(pe_tmp)\n",
        "        mi.append(pe_tmp + np.sum(np.array([np.sum(pred[:,i]*np.log(pred[:,i]))for i in range(0,n_classes)]))/len(pred))\n",
        "    \n",
        "    true = np.argmax(Y_test_run,axis=1)\n",
        "    pred = np.round(mean1)\n",
        "    \n",
        "    mean1_total.append(mean1)\n",
        "    pred_total.append(pred)\n",
        "    pred_true_total.append(true)\n",
        "    p_id_total.append(p_id_test_run)\n",
        "    sd1_total.append(sd1)\n",
        "    total_var_total.append(total_var)\n",
        "    vr_total.append(vr)\n",
        "    pe_total.append(pe)\n",
        "    mi_total.append(mi)\n",
        "\n",
        "    accuracy.append(acc(true,pred))\n",
        "    specificity.append(spec(true,pred))\n",
        "    sensitivity.append(sens(true,pred))\n",
        "    print('Accuracy: ', acc(true, pred))\n",
        "    print('Specificity: ', spec(true, pred))\n",
        "    print('Sensitivity: ', sens(true, pred))\n",
        "\n",
        "mean1_total = np.concatenate(mean1_total)\n",
        "pred_total = np.concatenate(pred_total)\n",
        "pred_true_total = np.concatenate(pred_true_total)\n",
        "p_id_total = np.concatenate(p_id_total)\n",
        "sd1_total = np.concatenate(sd1_total)\n",
        "total_var_total = np.concatenate(total_var_total)\n",
        "vr_total = np.concatenate(vr_total)\n",
        "pe_total = np.concatenate(pe_total)\n",
        "mi_total = np.concatenate(mi_total)\n",
        "    \n",
        "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'mean_pred':mean1_total, 'pat_true':pred_true_total,\n",
        "                   'sd':sd1_total, 'total_var':total_var_total, 'vr':vr_total, \n",
        "                    'pe':pe_total, 'mi':mi_total})\n",
        "dat.to_csv(folder + '/CV_predictions_pat_test_nn2_mc_dropout.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "nteract": {
      "version": "0.14.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}