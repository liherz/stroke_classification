{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1D-CNNs for 1Ch-NoMC\n",
        "- Image level predictions from 1Ch-NoMC\n",
        "- No MC dropout applied in the 1Ch-NoMC: no information about uncertainty in predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "tf.set_random_seed(3004)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = 'C:/Users/hezo/Dropbox/PhD/Stroke/Stroke_classification/Analyses_Oct_2018/outputs/5fold_CV_bl_without_mc_dropout/'\n",
        "folder = 'C:/Users/hezo/Documents/Stroke/patient_aggregation_1D_CNN_CV_bl_without_mc_dropout/'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#run 0\n",
        "train2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_train2.csv')\n",
        "valid2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_valid2.csv')\n",
        "test_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 1\n",
        "train2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_train2.csv')\n",
        "valid2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_valid2.csv')\n",
        "test_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 2\n",
        "train2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_train2.csv')\n",
        "valid2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_valid2.csv')\n",
        "test_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 3\n",
        "train2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_train2.csv')\n",
        "valid2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_valid2.csv')\n",
        "test_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 4\n",
        "train2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_train2.csv')\n",
        "valid2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_valid2.csv')\n",
        "test_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define accuracy, specificity and sensitivity\n",
        "def acc(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return (conf_mat[0][0]+conf_mat[1][1])/np.sum(conf_mat)\n",
        "def spec(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return conf_mat[0][0]/np.sum(conf_mat[0])\n",
        "def sens(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return conf_mat[1][1]/np.sum(conf_mat[1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and validation\n",
        "train = [train2_0, train2_1, train2_2, train2_3, train2_4]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for validing and validation\n",
        "valid = [valid2_0, valid2_1, valid2_2, valid2_3, valid2_4]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for testing and validation\n",
        "test = [test_0, test_1, test_2, test_3, test_4]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data to one hot"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToOneHot(vector, num_classes=None):\n",
        "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
        "    result[np.arange(len(vector)), vector] = 1\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1D-CNN: Some tries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.layers import Dense, Convolution1D, Input, Activation, Flatten, Dropout, GlobalMaxPool1D, MaxPooling1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(acc_train, acc_valid, loss_train, loss_valid):\n",
        "    plt.plot(acc_train, 'blue')\n",
        "    plt.plot(acc_valid, 'cyan')\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Train', 'Valid'], loc='lower right')\n",
        "    plt.show()\n",
        "    plt.plot(loss_train, 'blue')\n",
        "    plt.plot(loss_valid, 'cyan')\n",
        "    plt.ylim(0, 2.5)\n",
        "    plt.title('Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"Max val accuracy: \", np.max(acc_valid))\n",
        "    print(\"In epochs: \", np.where(acc_valid==np.max(acc_valid)))\n",
        "    print(\"Min val loss: \", np.min(loss_valid))\n",
        "    print('In epoch: ', np.where(loss_valid==np.min(loss_valid)))\n",
        "    return np.where(loss_valid==np.min(loss_valid))[0]+1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches_from_file(dat):\n",
        "    while True:\n",
        "        for p_id_tmp in set(dat.p_id):\n",
        "            # select one patient\n",
        "            pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
        "            # take all the predictions and save them in a list\n",
        "            X = pat_tmp.mean1.values\n",
        "            X = X.reshape((1,X.shape[0],1)) # 1 = batch, X.shape[0] = n images, 1 = n features\n",
        "            Y = pat_tmp.pat_true.head(n=1)\n",
        "            Y = convertToOneHot(Y.astype(int), 2)\n",
        "            #print(X, Y)\n",
        "            yield X, Y"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CV training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Run\", i)\n",
        "    \n",
        "    # Extract information for run i\n",
        "    train_run = train[i]\n",
        "    valid_run = valid[i]\n",
        "    \n",
        "    # Define model and compile\n",
        "    num_classes = 2\n",
        "    model = Sequential()\n",
        "    model.add(Convolution1D(16, kernel_size=3, activation=\"relu\", batch_input_shape=(None, None, 1)))\n",
        "    # model.add(Dropout(0.5))\n",
        "    # model.add(Convolution1D(8, kernel_size=3, activation=\"relu\"))\n",
        "    # model.add(Dropout(0.5))\n",
        "    # model.add(Convolution1D(8, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(GlobalMaxPool1D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    # Train the model and save checkpoints\n",
        "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn0/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "    results = model.fit_generator(generate_batches_from_file(train_run),\n",
        "                                  epochs=50,\n",
        "                                  steps_per_epoch=len(np.unique(train_run.p_id)),\n",
        "                                  verbose=1,\n",
        "                                  validation_data=generate_batches_from_file(valid_run),\n",
        "                                  validation_steps=len(np.unique(valid_run.p_id)),\n",
        "                                  callbacks=[cp_callback])\n",
        "                    \n",
        "    # save history\n",
        "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn0/history.csv', index=False)\n",
        "    \n",
        "    #### Find epoch with lowest validation loss\n",
        "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "accuracy = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "pred_total = []\n",
        "p_id_total = []\n",
        "pred_true_total = []\n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    print(\"Run\", i)\n",
        "    #### Extract information of run i\n",
        "    test_run = test[i]\n",
        "\n",
        "    # load the history\n",
        "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0/history.csv')\n",
        "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
        "    if epoch[0]<10:\n",
        "        model = load_model(folder + 'run'+str(i)+ '/nn0/model-0' + str(epoch[0]) + '.hdf5')\n",
        "    else:\n",
        "        model = load_model(folder + 'run'+str(i)+ '/nn0/model-' + str(epoch[0]) + '.hdf5')\n",
        "    pred_tmp = model.predict_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id)))\n",
        "    pred = np.argmax(pred_tmp, axis=1)\n",
        "    \n",
        "    # get the true labels from the generator\n",
        "    true = []\n",
        "    p_id =  []\n",
        "    for p_id_tmp in set(test_run.p_id):\n",
        "        # select one patient\n",
        "        pat_tmp = test_run.loc[test_run.p_id==p_id_tmp,:]\n",
        "        # take all the predictions and save them in a list\n",
        "        Y = pat_tmp.pat_true.head(n=1)\n",
        "        pid = pat_tmp.p_id.head(n=1)\n",
        "        true.append(Y.values[0])\n",
        "        p_id.append(pid.values[0])\n",
        "    \n",
        "    pred_total.append(pred_tmp[:,1])\n",
        "    pred_true_total.append(true)\n",
        "    p_id_total.append(p_id)\n",
        "\n",
        "    accuracy.append(acc(true,pred))\n",
        "    specificity.append(spec(true,pred))\n",
        "    sensitivity.append(sens(true,pred))\n",
        "    print('Accuracy: ', acc(true, pred))\n",
        "    print('Specificity: ', spec(true, pred))\n",
        "    print('Sensitivity: ', sens(true, pred))\n",
        "\n",
        "pred_total = np.concatenate(pred_total)\n",
        "pred_true_total = np.concatenate(pred_true_total)\n",
        "p_id_total = np.concatenate(p_id_total)\n",
        "    \n",
        "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'pat_true':pred_true_total})\n",
        "dat.to_csv(folder + '/CV_predictions_pat_test_nn0.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1D-CNN with MC Dropout: predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, Activation, Flatten, Dropout, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Run\", i)\n",
        "    \n",
        "    # Extract information for run i\n",
        "    train_run = train[i]\n",
        "    valid_run = valid[i]\n",
        "    \n",
        "    # Define model and compile\n",
        "    data_input = Input(shape=(None,1))\n",
        "    # Hidden layer\n",
        "    x = Convolution1D(16, kernel_size=3)(data_input)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    # x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    # x = Convolution1D(8, kernel_size=3)(x)\n",
        "    # x = Activation(\"relu\")(x)\n",
        "    # x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    # x = Convolution1D(8, kernel_size=3)(x)\n",
        "    # x = Activation(\"relu\")(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.5))(x)\n",
        "    x = Dense(num_classes)(x)\n",
        "    out = Activation(\"softmax\")(x)\n",
        "    \n",
        "    model = Model(inputs=data_input, outputs=out)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    # Train the model and save checkpoints\n",
        "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn0_mc/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "    results = model.fit_generator(generate_batches_from_file(train_run),\n",
        "                                  epochs=100,\n",
        "                                  steps_per_epoch=len(np.unique(train_run.p_id)),\n",
        "                                  verbose=1,\n",
        "                                  validation_data=generate_batches_from_file(valid_run),\n",
        "                                  validation_steps=len(np.unique(valid_run.p_id)),\n",
        "                                  callbacks=[cp_callback])\n",
        "                    \n",
        "    # save history\n",
        "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn0_mc/history.csv', index=False)\n",
        "    \n",
        "    #### Find epoch with lowest validation loss\n",
        "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "accuracy = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "pred_total = []\n",
        "p_id_total = []\n",
        "pred_true_total = []\n",
        "sd1_total = []\n",
        "total_var_total = []\n",
        "vr_total = []\n",
        "pe_total = []\n",
        "mi_total = []\n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    print(\"Run\", i)\n",
        "    #### Extract information of run i\n",
        "    test_run = test[i]\n",
        "\n",
        "    # load the history\n",
        "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0_mc/history.csv')\n",
        "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
        "    if epoch[0]<10:\n",
        "        model = load_model(folder + 'run'+str(i)+ '/nn0_mc/model-0' + str(epoch[0]) + '.hdf5')\n",
        "    else:\n",
        "        model = load_model(folder + 'run'+str(i)+ '/nn0_mc/model-' + str(epoch[0]) + '.hdf5')\n",
        "    \n",
        "    # get 200 predictions for each patient\n",
        "    raw_pred = np.empty([500,len(np.unique(test_run.p_id)),2])\n",
        "    for i in range(500):\n",
        "        raw_pred[i] = model.predict_generator(generate_batches_from_file(test_run), steps=len(np.unique(test_run.p_id)))\n",
        "    # get the mean over all predictions\n",
        "    pred_tmp = np.mean(raw_pred,axis=0)\n",
        "    pred = np.argmax(pred_tmp, axis=1)\n",
        "\n",
        "    # get the true labels from the generator\n",
        "    # save the variances\n",
        "    sd0_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,0]\n",
        "    sd1_tmp = np.array(np.std(raw_pred, ddof=1, axis=0))[:,1]\n",
        "    total_var_total.append(sd0_tmp**2 + sd1_tmp**2)\n",
        "    sd1_total.append(sd1_tmp)\n",
        "    \n",
        "    raw_pred[raw_pred==0]=1e-40\n",
        "    j=0\n",
        "    \n",
        "    true = []\n",
        "    p_id =  []\n",
        "    vr = []\n",
        "    pe = []\n",
        "    mi = []\n",
        "    for p_id_tmp in set(test_run.p_id):\n",
        "        # select one patient\n",
        "        pat_tmp = test_run.loc[test_run.p_id==p_id_tmp,:]\n",
        "        # take all the predictions and save them in a list\n",
        "        Y = pat_tmp.pat_true.head(n=1)\n",
        "        pid = pat_tmp.p_id.head(n=1)\n",
        "        true.append(Y.values[0])\n",
        "        p_id.append(pid.values[0])\n",
        "        \n",
        "        vr.append(1-(np.max(np.histogram(np.argmax(raw_pred[:,j,:], axis=1), bins=n_classes, range=[0,n_classes])[0])/len(raw_pred[:,j,:])))\n",
        "        pe_tmp = (-1)*np.sum(np.mean(raw_pred[:,j,:], axis=0)*np.log(np.mean(raw_pred[:,j,:], axis=0)))\n",
        "        pe.append(pe_tmp)\n",
        "        mi.append(pe_tmp + np.sum(np.array([np.sum(raw_pred[:,j,i]*np.log(raw_pred[:,j,i]))for i in range(0,n_classes)]))/len(raw_pred[:,j,:]))\n",
        "        j = j+1\n",
        "        \n",
        "    \n",
        "    pred_total.append(pred_tmp[:,1])\n",
        "    pred_true_total.append(true)\n",
        "    p_id_total.append(p_id)\n",
        "    vr_total.append(vr)\n",
        "    pe_total.append(pe)\n",
        "    mi_total.append(mi)\n",
        "    \n",
        "    accuracy.append(acc(true,pred))\n",
        "    specificity.append(spec(true,pred))\n",
        "    sensitivity.append(sens(true,pred))\n",
        "    print('Accuracy: ', acc(true, pred))\n",
        "    print('Specificity: ', spec(true, pred))\n",
        "    print('Sensitivity: ', sens(true, pred))\n",
        "\n",
        "pred_total = np.concatenate(pred_total)\n",
        "pred_true_total = np.concatenate(pred_true_total)\n",
        "p_id_total = np.concatenate(p_id_total)\n",
        "sd1_total = np.concatenate(sd1_total)\n",
        "total_var_total = np.concatenate(total_var_total)\n",
        "vr_total = np.concatenate(vr_total)\n",
        "pe_total = np.concatenate(pe_total)\n",
        "mi_total = np.concatenate(mi_total)\n",
        "    \n",
        "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'pat_true':pred_true_total,\n",
        "                   'total_var':total_var_total, 'sd1':sd1_total, 'vr':vr_total, 'pe':pe_total,\n",
        "                   'mi':mi_total})\n",
        "dat.to_csv(folder + '/CV_predictions_pat_test_nn0_mc.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "nteract": {
      "version": "0.14.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}