{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks for BL-CNN"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "tf.set_random_seed(3004)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = 'C:/Users/hezo/Dropbox/PhD/Stroke/Stroke_classification/Analyses_Oct_2018/outputs/5fold_CV_bl_without_mc_dropout/'\n",
        "folder = 'C:/Users/hezo/Documents/Stroke/patient_aggregation_NN_CV_bl_ohne_mc_dropout_v2/'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#run 0\n",
        "train2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_train2.csv')\n",
        "valid2_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_valid2.csv')\n",
        "test_0 = pd.read_csv(data_folder + 'run0/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 1\n",
        "train2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_train2.csv')\n",
        "valid2_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_valid2.csv')\n",
        "test_1 = pd.read_csv(data_folder + 'run1/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 2\n",
        "train2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_train2.csv')\n",
        "valid2_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_valid2.csv')\n",
        "test_2 = pd.read_csv(data_folder + 'run2/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 3\n",
        "train2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_train2.csv')\n",
        "valid2_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_valid2.csv')\n",
        "test_3 = pd.read_csv(data_folder + 'run3/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run 4\n",
        "train2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_train2.csv')\n",
        "valid2_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_valid2.csv')\n",
        "test_4 = pd.read_csv(data_folder + 'run4/predictions_dropout_test.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define accuracy, specificity and sensitivity\n",
        "def acc(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return (conf_mat[0][0]+conf_mat[1][1])/np.sum(conf_mat)\n",
        "def spec(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return conf_mat[0][0]/np.sum(conf_mat[0])\n",
        "def sens(true, pred):\n",
        "    conf_mat = confusion_matrix(true, pred)\n",
        "    return conf_mat[1][1]/np.sum(conf_mat[1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consider the x images which most likely show a stroke"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes which contain the information (p, pe, vr, mi)\n",
        "# of the 5 images which most likely show a stroke\n",
        "def reshape_data(dat, n):\n",
        "    # Define image and label\n",
        "    X = np.zeros((len(set(dat.p_id)), n, 5), dtype=np.float32)\n",
        "    print(X.shape)\n",
        "    Y = np.zeros(len(set(dat.p_id)))\n",
        "    p_id = np.zeros(len(set(dat.p_id)))\n",
        "    j = 0\n",
        "    for p_id_tmp in set(dat.p_id):\n",
        "        # select one patient and  take the n images with highest prob\n",
        "        pat_tmp = dat.loc[dat.p_id==p_id_tmp,:]\n",
        "        pat_tmp_sorted = pat_tmp.sort_values(by=['mean1'], ascending=False)\n",
        "        pat_tmp_sorted = pat_tmp_sorted.head(n=n)\n",
        "        X[j] = pat_tmp_sorted.loc[:,['mean1','pe','vr','mi','total_var']]\n",
        "        Y[j] = pat_tmp_sorted.pat_true.head(n=1)\n",
        "        p_id[j] = pat_tmp_sorted.p_id.head(n=1)\n",
        "        j = j+1\n",
        "    Y = Y.astype(int)\n",
        "    return X, Y, p_id"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_images = 5"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_0, Y_train2_0, p_id_train2_0 = reshape_data(train2_0, n_images)\n",
        "print('train2: ', X_train2_0.shape, Y_train2_0.shape)\n",
        "X_valid2_0, Y_valid2_0, p_id_valid2_0 = reshape_data(valid2_0, n_images)\n",
        "print('Valid2: ', X_valid2_0.shape, Y_valid2_0.shape)\n",
        "X_test_0, Y_test_0, p_id_test_0 = reshape_data(test_0, n_images)\n",
        "print('test: ', X_test_0.shape, Y_test_0.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_1, Y_train2_1, p_id_train2_1 = reshape_data(train2_1, n_images)\n",
        "print('train2: ', X_train2_1.shape, Y_train2_1.shape)\n",
        "X_valid2_1, Y_valid2_1, p_id_valid2_1 = reshape_data(valid2_1, n_images)\n",
        "print('Valid2: ', X_valid2_1.shape, Y_valid2_1.shape)\n",
        "X_test_1, Y_test_1, p_id_test_1 = reshape_data(test_1, n_images)\n",
        "print('test: ', X_test_1.shape, Y_test_1.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_2, Y_train2_2, p_id_train2_2 = reshape_data(train2_2, n_images)\n",
        "print('train2: ', X_train2_2.shape, Y_train2_2.shape)\n",
        "X_valid2_2, Y_valid2_2, p_id_valid2_2 = reshape_data(valid2_2, n_images)\n",
        "print('Valid2: ', X_valid2_2.shape, Y_valid2_2.shape)\n",
        "X_test_2, Y_test_2, p_id_test_2 = reshape_data(test_2, n_images)\n",
        "print('test: ', X_test_2.shape, Y_test_2.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_3, Y_train2_3, p_id_train2_3 = reshape_data(train2_3, n_images)\n",
        "print('train2: ', X_train2_3.shape, Y_train2_3.shape)\n",
        "X_valid2_3, Y_valid2_3, p_id_valid2_3 = reshape_data(valid2_3, n_images)\n",
        "print('Valid2: ', X_valid2_3.shape, Y_valid2_3.shape)\n",
        "X_test_3, Y_test_3, p_id_test_3 = reshape_data(test_3, n_images)\n",
        "print('test: ', X_test_3.shape, Y_test_3.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2_4, Y_train2_4, p_id_train2_4 = reshape_data(train2_4, n_images)\n",
        "print('train2: ', X_train2_4.shape, Y_train2_4.shape)\n",
        "X_valid2_4, Y_valid2_4, p_id_valid2_4 = reshape_data(valid2_4, n_images)\n",
        "print('Valid2: ', X_valid2_4.shape, Y_valid2_4.shape)\n",
        "X_test_4, Y_test_4, p_id_test_4 = reshape_data(test_4, n_images)\n",
        "print('test: ', X_test_4.shape, Y_test_4.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# in test4 we have one patient more, leave this out here, makes it little more complicated to program\n",
        "X_test_4 = X_test_4[:102]\n",
        "Y_test_4 = Y_test_4[:102]\n",
        "p_id_test_4 = p_id_test_4[:102]\n",
        "print('test: ', X_test_4.shape, Y_test_4.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and validation\n",
        "X_train = np.empty((5,X_train2_0.shape[0],X_train2_0.shape[1],X_train2_0.shape[2]))\n",
        "X_train[0] = X_train2_0\n",
        "X_train[1] = X_train2_1\n",
        "X_train[2] = X_train2_2\n",
        "X_train[3] = X_train2_3\n",
        "X_train[4] = X_train2_4\n",
        "\n",
        "Y_train = np.empty((5,Y_train2_0.shape[0]))\n",
        "Y_train[0] = Y_train2_0\n",
        "Y_train[1] = Y_train2_1\n",
        "Y_train[2] = Y_train2_2\n",
        "Y_train[3] = Y_train2_3\n",
        "Y_train[4] = Y_train2_4\n",
        "\n",
        "p_id_train = np.empty((5,p_id_train2_0.shape[0]))\n",
        "p_id_train[0] = p_id_train2_0\n",
        "p_id_train[1] = p_id_train2_1\n",
        "p_id_train[2] = p_id_train2_2\n",
        "p_id_train[3] = p_id_train2_3\n",
        "p_id_train[4] = p_id_train2_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and validation\n",
        "X_valid = np.empty((5,X_valid2_0.shape[0],X_valid2_0.shape[1],X_valid2_0.shape[2]))\n",
        "X_valid[0] = X_valid2_0\n",
        "X_valid[1] = X_valid2_1\n",
        "X_valid[2] = X_valid2_2\n",
        "X_valid[3] = X_valid2_3\n",
        "X_valid[4] = X_valid2_4\n",
        "\n",
        "Y_valid = np.empty((5,Y_valid2_0.shape[0]))\n",
        "Y_valid[0] = Y_valid2_0\n",
        "Y_valid[1] = Y_valid2_1\n",
        "Y_valid[2] = Y_valid2_2\n",
        "Y_valid[3] = Y_valid2_3\n",
        "Y_valid[4] = Y_valid2_4\n",
        "\n",
        "p_id_valid = np.empty((5,p_id_valid2_0.shape[0]))\n",
        "p_id_valid[0] = p_id_valid2_0\n",
        "p_id_valid[1] = p_id_valid2_1\n",
        "p_id_valid[2] = p_id_valid2_2\n",
        "p_id_valid[3] = p_id_valid2_3\n",
        "p_id_valid[4] = p_id_valid2_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data which we use for training and testation\n",
        "X_test = np.empty((5,X_test_0.shape[0],X_test_0.shape[1],X_test_0.shape[2]))\n",
        "X_test[0] = X_test_0\n",
        "X_test[1] = X_test_1\n",
        "X_test[2] = X_test_2\n",
        "X_test[3] = X_test_3\n",
        "X_test[4] = X_test_4\n",
        "\n",
        "Y_test = np.empty((5,Y_test_0.shape[0]))\n",
        "Y_test[0] = Y_test_0\n",
        "Y_test[1] = Y_test_1\n",
        "Y_test[2] = Y_test_2\n",
        "Y_test[3] = Y_test_3\n",
        "Y_test[4] = Y_test_4\n",
        "\n",
        "p_id_test = np.empty((5,p_id_test_0.shape[0]))\n",
        "p_id_test[0] = p_id_test_0\n",
        "p_id_test[1] = p_id_test_1\n",
        "p_id_test[2] = p_id_test_2\n",
        "p_id_test[3] = p_id_test_3\n",
        "p_id_test[4] = p_id_test_4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_valid.shape, X_test.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize inputs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_norm = np.empty((X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
        "X_valid_norm = np.empty((X_valid.shape[0], X_valid.shape[1], X_valid.shape[2], X_valid.shape[3]))\n",
        "X_test_norm = np.empty((X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3]))\n",
        "for i in range(5):\n",
        "    X_mean = np.mean(X_train[i], axis=0)\n",
        "    X_std = np.std(X_train[i], axis=0)\n",
        "    X_train_norm[i] = (X_train[i] - X_mean)/(X_std+0.00001)\n",
        "    X_valid_norm[i] = (X_valid[i] - X_mean)/(X_std+0.00001)\n",
        "    X_test_norm[i] = (X_test[i] - X_mean)/(X_std+0.00001)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data to one hot"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToOneHot(vector, num_classes=None):\n",
        "    result = np.zeros((len(vector), num_classes), dtype='int32')\n",
        "    result[np.arange(len(vector)), vector] = 1\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_new = np.empty((5,Y_train.shape[1],2))\n",
        "Y_valid_new = np.empty((5,Y_valid.shape[1],2))\n",
        "Y_test_new = np.empty((5,Y_test.shape[1],2))\n",
        "for i in range(5):\n",
        "    Y_train_new[i] = convertToOneHot(Y_train[i].astype(int), 2)\n",
        "    Y_valid_new[i] = convertToOneHot(Y_valid[i].astype(int), 2)\n",
        "    Y_test_new[i] = convertToOneHot(Y_test[i].astype(int), 2)\n",
        "print(Y_train.shape, Y_valid.shape, Y_test.shape)\n",
        "print(Y_train_new.shape, Y_valid_new.shape, Y_test_new.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FC-NN with MC Dropout: Predictions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, Activation, Flatten, Dropout, Lambda\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as K"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(acc_train, acc_valid, loss_train, loss_valid):\n",
        "    plt.plot(acc_train, 'blue')\n",
        "    plt.plot(acc_valid, 'cyan')\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Train', 'Valid'], loc='lower right')\n",
        "    plt.show()\n",
        "    plt.plot(loss_train, 'blue')\n",
        "    plt.plot(loss_valid, 'cyan')\n",
        "    plt.ylim(0, 2.5)\n",
        "    plt.title('Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper right')\n",
        "    plt.show()\n",
        "    print(\"Max val accuracy: \", np.max(acc_valid))\n",
        "    print(\"In epochs: \", np.where(acc_valid==np.max(acc_valid)))\n",
        "    print(\"Min val loss: \", np.min(loss_valid))\n",
        "    print('In epoch: ', np.where(loss_valid==np.min(loss_valid)))\n",
        "    return np.where(loss_valid==np.min(loss_valid))[0]+1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Run\", i)\n",
        "    \n",
        "    #### Extract information of run i\n",
        "    X_train_run = X_train_norm[i]\n",
        "    Y_train_run = Y_train_new[i]\n",
        "    p_id_train_run = p_id_train[i]\n",
        "    X_valid_run = X_valid_norm[i]\n",
        "    Y_valid_run = Y_valid_new[i]\n",
        "    p_id_valid_run = p_id_valid[i]\n",
        "    \n",
        "    #### define the model\n",
        "    # Inputs: p\n",
        "    data_input = Input(shape=(5,))\n",
        "    # Hidden layer\n",
        "    x = Dense(8)(data_input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    x = Dense(8)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Lambda(lambda x: K.dropout(x, level=0.3))(x)\n",
        "    # output\n",
        "    out = Dense(2, activation='softmax', name='output')(x)\n",
        "    model = Model(inputs=data_input, outputs=out)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    #### Consider only the predictions\n",
        "    X_train_pred = X_train_run[:,:,0]\n",
        "    X_valid_pred = X_valid_run[:,:,0]\n",
        "    print(X_train_pred.shape, X_valid_pred.shape)\n",
        "    \n",
        "    #### Train the model and save checkpoints\n",
        "    cp_callback = ModelCheckpoint(folder + 'run'+str(i)+'/nn0/model-{epoch:02d}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "    results = model.fit(X_train_pred, Y_train_run,\n",
        "                    batch_size=2, \n",
        "                    epochs=200, \n",
        "                    validation_data=(X_valid_pred, Y_valid_run),\n",
        "                    callbacks=[cp_callback])\n",
        "    # save history\n",
        "    pd.DataFrame(results.history).to_csv(folder + 'run'+str(i)+'/nn0/history.csv', index=False)\n",
        "    \n",
        "    #### Find epoch with lowest validation loss\n",
        "    epoch = show_results(results.history['acc'], results.history['val_acc'], results.history['loss'], results.history['val_loss'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction: Test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the results on the test\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "sensitivity = []\n",
        "specificity = []\n",
        "mean1_total = []\n",
        "pred_total = []\n",
        "p_id_total = []\n",
        "pred_true_total = []\n",
        "sd1_total = []\n",
        "total_var_total = []\n",
        "vr_total = []\n",
        "pe_total = []\n",
        "mi_total = []  \n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    print(\"Run\", i)\n",
        "    #### Extract information of run i\n",
        "    X_test_run = X_test_norm[i]\n",
        "    Y_test_run = Y_test_new[i]\n",
        "    p_id_test_run = p_id_test[i]\n",
        "    \n",
        "    X_test_pred = X_test_run[:,:,0]\n",
        "    print(X_test_pred.shape)\n",
        "    \n",
        "    #### Start a new session\n",
        "    print('Start new session for predictions')\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    K.set_session(sess)\n",
        "    \n",
        "    \n",
        "    # load the history\n",
        "    dat = pd.DataFrame.from_csv(folder + 'run'+str(i)+'/nn0/history.csv')\n",
        "    epoch = np.where(dat.val_loss==np.min(dat.val_loss))[0]+1\n",
        "    if epoch[0]<10:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn0/model-0' + str(epoch[0]) + '.hdf5')\n",
        "    else:\n",
        "        model2 = load_model(folder + 'run'+str(i)+ '/nn0/model-' + str(epoch[0]) + '.hdf5')\n",
        "    # pred = np.argmax(model.predict(X_test_pred), axis=1)\n",
        "    # true = np.argmax(Y_test_run, axis=1)\n",
        "    \n",
        "    print('Get the predictions')\n",
        "    n_classes = 2\n",
        "    \n",
        "    predictions = np.zeros((len(X_test_pred), 500, n_classes))\n",
        "    mean1 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd0 = np.zeros((X_test_pred.shape[0]))\n",
        "    sd1 = np.zeros((X_test_pred.shape[0]))\n",
        "    total_var = np.zeros((X_test_pred.shape[0]))\n",
        "    vr = []\n",
        "    pe = []\n",
        "    mi = [] \n",
        "    for j in range(len(X_test_pred)):\n",
        "        # repeat the current image 500 times\n",
        "        X_rep = np.empty((500,X_test_pred.shape[1]))\n",
        "        X_rep[:] = X_test_pred[j:j+1]\n",
        "        # get 500 predictions for this image\n",
        "        pred = sess.run(model2.output, feed_dict={model2.input: X_rep})\n",
        "        \n",
        "        # output of mean and sd == #classes\n",
        "        predictions[j] = pred # save the raw predictions\n",
        "        mean1[j] = np.mean(pred, axis=0)[1]\n",
        "        sd0[j], sd1[j] = np.array(np.std(pred, ddof=1, axis=0))\n",
        "        total_var[j] = sd0[j]**2 + sd1[j]**2\n",
        "        pred[pred==0]=1e-40\n",
        "        vr.append(1-(np.max(np.histogram(np.argmax(pred, axis=1), bins=n_classes, range=[0,n_classes])[0])/len(pred)))\n",
        "        pe_tmp = (-1)*np.sum(np.mean(pred, axis=0)*np.log(np.mean(pred, axis=0)))\n",
        "        pe.append(pe_tmp)\n",
        "        mi.append(pe_tmp + np.sum(np.array([np.sum(pred[:,i]*np.log(pred[:,i]))for i in range(0,n_classes)]))/len(pred))\n",
        "    \n",
        "    true = np.argmax(Y_test_run,axis=1)\n",
        "    pred = np.round(mean1)\n",
        "    \n",
        "    mean1_total.append(mean1)\n",
        "    pred_total.append(pred)\n",
        "    pred_true_total.append(true)\n",
        "    p_id_total.append(p_id_test_run)\n",
        "    sd1_total.append(sd1)\n",
        "    total_var_total.append(total_var)\n",
        "    vr_total.append(vr)\n",
        "    pe_total.append(pe)\n",
        "    mi_total.append(mi)\n",
        "\n",
        "    accuracy.append(acc(true,pred))\n",
        "    specificity.append(spec(true,pred))\n",
        "    sensitivity.append(sens(true,pred))\n",
        "    print('Accuracy: ', acc(true, pred))\n",
        "    print('Specificity: ', spec(true, pred))\n",
        "    print('Sensitivity: ', sens(true, pred))\n",
        "\n",
        "mean1_total = np.concatenate(mean1_total)\n",
        "pred_total = np.concatenate(pred_total)\n",
        "pred_true_total = np.concatenate(pred_true_total)\n",
        "p_id_total = np.concatenate(p_id_total)\n",
        "sd1_total = np.concatenate(sd1_total)\n",
        "total_var_total = np.concatenate(total_var_total)\n",
        "vr_total = np.concatenate(vr_total)\n",
        "pe_total = np.concatenate(pe_total)\n",
        "mi_total = np.concatenate(mi_total)\n",
        "    \n",
        "dat = pd.DataFrame({'p_id':p_id_total, 'pred':pred_total, 'mean_pred':mean1_total, 'pat_true':pred_true_total,\n",
        "                   'sd':sd1_total, 'total_var':total_var_total, 'vr':vr_total, \n",
        "                    'pe':pe_total, 'mi':mi_total})\n",
        "dat.to_csv(folder + '/CV_predictions_pat_test_nn0_mc_dropout.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "nteract": {
      "version": "0.14.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}